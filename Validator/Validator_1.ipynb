{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, make_scorer, recall_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Features = [\n",
    "    \"ticker\", \"company_name\", \"sector\", \"industry\", \"market_cap\",\n",
    "    \"price\", \"open\", \"close\", \"high\", \"low\",\n",
    "    \"volume\", \"adj_close\", \"dividend\", \"dividend_yield\", \"pe_ratio\",\n",
    "    \"eps\", \"beta\", \"52_week_high\", \"52_week_low\", \"shares_outstanding\",\n",
    "    \"float\", \"average_volume\", \"market\", \"exchange\", \"isin\",\n",
    "    \"cusip\", \"country\", \"currency\", \"ipo_date\", \"earnings_date\",\n",
    "    \"revenue\", \"cost_of_goods_sold\", \"gross_profit\", \"operating_expenses\", \"operating_income\",\n",
    "    \"ebit\", \"ebitda\", \"net_income\", \"income_before_tax\", \"tax_expense\",\n",
    "    \"net_income_applicable_to_common_shares\", \"basic_eps\", \"diluted_eps\", \"total_assets\", \"current_assets\",\n",
    "    \"non_current_assets\", \"total_liabilities\", \"current_liabilities\", \"non_current_liabilities\", \"shareholders_equity\",\n",
    "    \"retained_earnings\", \"cash_and_cash_equivalents\", \"short_term_investments\", \"long_term_investments\", \"inventory\",\n",
    "    \"accounts_receivable\", \"accounts_payable\", \"depreciation\", \"amortization\", \"capital_expenditures\",\n",
    "    \"loan_id\", \"loan_amount\", \"loan_term\", \"interest_rate\", \"installment\",\n",
    "    \"issue_date\", \"loan_status\", \"payment_status\", \"borrower_score\", \"borrower_income\",\n",
    "    \"debt_to_income\", \"employment_length\", \"purpose\", \"home_ownership\", \"delinquency_2yrs\",\n",
    "    \"credit_score\", \"fico_range_low\", \"fico_range_high\", \"revol_util\", \"num_open_credit_lines\",\n",
    "    \"total_credit_lines\", \"public_records\", \"collections_12_mths_ex_med\", \"application_type\", \"verification_status\",\n",
    "    \"bond_id\", \"bond_name\", \"maturity_date\", \"coupon_rate\", \"yield_to_maturity\",\n",
    "    \"face_value\", \"issue_price\", \"current_price\", \"duration\", \"convexity\",\n",
    "    \"credit_rating\", \"issuer\", \"callable\", \"puttable\", \"bond_type\",\n",
    "    \"transaction_id\", \"transaction_date\", \"transaction_amount\", \"transaction_type\", \"merchant_name\",\n",
    "    \"merchant_category\", \"account_id\", \"balance_before\", \"balance_after\", \"location\",\n",
    "    \"crypto_symbol\", \"crypto_name\", \"market_rank\", \"circulating_supply\", \"total_supply\",\n",
    "    \"max_supply\", \"market_dominance\", \"all_time_high\", \"all_time_low\", \"last_updated\",\n",
    "    \"block_time\", \"hashing_algorithm\", \"platform\", \"explorer_url\", \"trading_pairs\",\n",
    "    \"exchange_rate\", \"currency_pair\", \"base_currency\", \"quote_currency\", \"rate_date\",\n",
    "    \"rate_time\", \"daily_change\", \"monthly_change\", \"yearly_change\", \"volume_24h\",\n",
    "    \"investment_id\", \"investment_type\", \"investment_amount\", \"investment_date\", \"current_value\",\n",
    "    \"gain_loss\", \"annual_return\", \"investment_duration\", \"investment_strategy\", \"fund_manager\",\n",
    "    \"fund_id\", \"fund_name\", \"nav\", \"expense_ratio\", \"inception_date\",\n",
    "    \"fund_category\", \"assets_under_management\", \"benchmark_index\", \"turnover_ratio\", \"dividend_distribution\",\n",
    "    \"gdp\", \"inflation_rate\", \"unemployment_rate\", \"federal_funds_rate\", \"consumer_price_index\",\n",
    "    \"producer_price_index\", \"retail_sales\", \"housing_starts\", \"trade_balance\", \"government_debt\",\n",
    "    \"current_account_balance\", \"budget_deficit\", \"foreign_reserves\", \"money_supply\", \"taxpayer_id\",\n",
    "    \"income_bracket\", \"taxable_income\", \"effective_tax_rate\", \"tax_paid\", \"deductions\",\n",
    "    \"credits\", \"filing_status\", \"tax_year\", \"refund_amount\", \"bank_id\",\n",
    "    \"branch_id\", \"account_type\", \"account_open_date\", \"account_balance\", \"interest_earned\",\n",
    "    \"overdraft_limit\", \"minimum_balance\", \"monthly_fee\", \"account_status\", \"user_id\",\n",
    "    \"customer_id\", \"registration_date\", \"last_login\", \"kyc_status\", \"risk_score\",\n",
    "    \"fraud_flag\", \"device_id\", \"ip_address\", \"login_location\", \"portfolio_id\",\n",
    "    \"asset_class\", \"allocation_percentage\", \"benchmark_return\", \"tracking_error\", \"sharpe_ratio\",\n",
    "    \"alpha\", \"beta_coefficient\", \"standard_deviation\", \"max_drawdown\", \"audit_status\", \"accounting_standard\", \"financial_statement_type\", \"reporting_currency\", \"adjustment_reason\",\n",
    "    \"deferred_tax_assets\", \"deferred_tax_liabilities\", \"intangible_assets\", \"goodwill\", \"preferred_equity\",\n",
    "    \"policy_id\", \"policy_holder\", \"premium_amount\", \"coverage_amount\", \"claim_id\",\n",
    "    \"claim_status\", \"underwriting_score\", \"risk_class\", \"loss_ratio\", \"combined_ratio\",\n",
    "    \"order_id\", \"trade_price\", \"trade_volume\", \"order_type\", \"execution_time\",\n",
    "    \"bid_price\", \"ask_price\", \"spread\", \"order_book_depth\", \"trading_halt\",\n",
    "    \"swift_code\", \"iban\", \"routing_number\", \"account_opening_method\", \"branch_location\",\n",
    "    \"atm_withdrawals\", \"wire_transfers\", \"monthly_statements\", \"account_tier\", \"fee_structure\",\n",
    "    \"credit_limit\", \"credit_line_type\", \"charge_off_status\", \"days_past_due\", \"collection_agency\",\n",
    "    \"restructuring_status\", \"forbearance_flag\", \"loan_purpose\", \"collateral_type\", \"repayment_behavior\",\n",
    "    \"employment_rate\", \"labor_force_participation\", \"consumer_confidence_index\", \"housing_index\", \"manufacturing_index\",\n",
    "    \"import_volume\", \"export_volume\", \"interest_payment\", \"sovereign_rating\", \"external_debt\",\n",
    "    \"payment_method\", \"payment_gateway\", \"settlement_status\", \"refund_status\", \"dispute_id\",\n",
    "    \"chargeback_amount\", \"recurring_payment\", \"subscription_id\", \"billing_cycle\", \"invoice_date\",\n",
    "    \"wealth_segment\", \"advisor_id\", \"fee_schedule\", \"client_risk_profile\", \"discretionary_mandate\",\n",
    "    \"goals_based_plan\", \"financial_goal\", \"investment_objective\", \"cash_allocation\", \"equity_allocation\",\n",
    "    \"esg_score\", \"carbon_emission\", \"sustainability_rating\", \"board_diversity\", \"executive_compensation_ratio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Features = [\n",
    "   \"revenue_growth\", \"net_margin\", \"operating_margin\", \"book_value_per_share\", \"enterprise_value\",\n",
    "   \"ev_to_ebitda\", \"price_to_free_cash_flow\", \"fcf_margin\", \"roic\", \"roa\",\n",
    "   \"cash_conversion_cycle\", \"interest_coverage_ratio\", \"days_payable_outstanding\", \"inventory_turnover\", \"quick_ratio\",\n",
    "   \"z_score\", \"altman_z_score\", \"short_interest_ratio\", \"put_call_ratio\", \"analyst_recommendation\",\n",
    "   \"price_target_high\", \"price_target_low\", \"estimate_revision\", \"guidance_change\", \"buyback_yield\",\n",
    "   \"s&p_rating\", \"moody_rating\", \"recovery_rate\", \"default_probability\", \"credit_spread\",\n",
    "   \"real_interest_rate\", \"velocity_of_money\", \"consumer_sentiment_index\", \"labor_cost_index\", \"construction_spending\",\n",
    "   \"crypto_funding_rate\", \"staking_yield\", \"token_burn_rate\", \"dao_votes\", \"mining_difficulty\",\n",
    "   \"digital_wallet_id\", \"transaction_fee\", \"payment_token\", \"subscription_status\", \"auto_renew_flag\",\n",
    "   \"claim_frequency\", \"premium_to_coverage_ratio\", \"lapse_rate\", \"policy_duration\", \"actuarial_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define preprocessing function\n",
    "def preprocess_keyword(keyword):\n",
    "    return keyword.replace(\"_\", \" \").lower()\n",
    "\n",
    "\n",
    "# Load FinancialBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "# Function to generate embeddings using FinancialBERT\n",
    "def generate_embeddings(keywords, tokenizer, model, device):\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_Features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m keyword.replace(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m).lower()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Preprocess training and testing features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m processed_keywords = [preprocess_keyword(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mTrain_Features\u001b[49m]\n\u001b[32m      7\u001b[39m processed_testing_keywords = [preprocess_keyword(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m Test_Features]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load FinancialBERT tokenizer and model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Train_Features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess training and testing features\n",
    "processed_keywords = [preprocess_keyword(k) for k in Train_Features]\n",
    "processed_testing_keywords = [preprocess_keyword(k) for k in Test_Features]\n",
    "\n",
    "\n",
    "# Generate embeddings for training and testing data\n",
    "embeddings = generate_embeddings(processed_keywords, tokenizer, model, device)\n",
    "testing_embeddings = generate_embeddings(processed_testing_keywords, tokenizer, model, device)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Perform t-SNE visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "# Annotate each point with its corresponding label\n",
    "for i, label in enumerate(processed_keywords):\n",
    "    plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=7, alpha=0.8)\n",
    "\n",
    "# Add title and grid\n",
    "plt.title(\"t-SNE Visualization of Financial Keyword Embeddings using FinancialBERT\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping threshold 0.800 due to positive ratio: 0.2489\n",
      "‚ö†Ô∏è Skipping threshold 0.810 due to positive ratio: 0.2062\n",
      "‚ö†Ô∏è Skipping threshold 0.820 due to positive ratio: 0.1683\n",
      "‚ö†Ô∏è Skipping threshold 0.830 due to positive ratio: 0.1335\n",
      "‚ö†Ô∏è Skipping threshold 0.840 due to positive ratio: 0.1016\n"
     ]
    }
   ],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42, gamma='scale')\n",
    "\n",
    "threshold_candidates = np.arange(0.8, 0.9, 0.01)\n",
    "\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "\n",
    "min_positives_ratio = 0.01\n",
    "max_positives_ratio = 0.10\n",
    "\n",
    "\n",
    "for threshold in threshold_candidates:\n",
    "    X_temp, y_temp = [], []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            vec1, vec2 = embeddings[i], embeddings[j]\n",
    "            cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            label = 1 if cos_sim > threshold else 0\n",
    "            X_temp.append(np.concatenate([vec1, vec2]))\n",
    "            y_temp.append(label)\n",
    "\n",
    "    X_temp, y_temp = np.array(X_temp), np.array(y_temp)\n",
    "    positives_ratio = np.mean(y_temp)\n",
    "\n",
    "    # Skip thresholds with too few or too many positives\n",
    "    if not (min_positives_ratio <= positives_ratio <= max_positives_ratio):\n",
    "        print(f\"‚ö†Ô∏è Skipping threshold {threshold:.3f} due to positive ratio: {positives_ratio:.4f}\")\n",
    "        continue\n",
    "\n",
    "    # Split and scale\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "    X_val_scaled = scaler.transform(X_val_sub)\n",
    "\n",
    "    # Train and evaluate\n",
    "    svm_model.fit(X_train_scaled, y_train_sub)\n",
    "    y_pred_val = svm_model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_sub, y_pred_val, zero_division=0)\n",
    "\n",
    "    print(f\"‚úÖ Threshold: {threshold:.3f} | F1: {f1:.4f} | Positive Ratio: {positives_ratio:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "if best_threshold is not None:\n",
    "    print(f\"\\nüéØ Best Threshold Found: {best_threshold:.3f} with F1-score: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No valid threshold found within ratio constraints. Try adjusting the limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X = []  # Input features (concatenated embeddings)\n",
    "y = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        combined_features = np.concatenate([embeddings[i], embeddings[j]])  # Concatenate embeddings\n",
    "        X.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        y.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Original pairs: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 29169\n",
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "def jitter_embedding(embedding, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, embedding.shape)\n",
    "    return embedding + noise\n",
    "\n",
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(embeddings[i])\n",
    "                vec2_jit = jitter_embedding(embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X)}\")\n",
    "# Combine original and augmented data\n",
    "X_combined = np.vstack([X, np.array(augmented_pairs)])\n",
    "y_combined = np.concatenate([y, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X_Test = []  # Input features (concatenated embeddings)\n",
    "y_test = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        combined_features = np.concatenate([testing_embeddings[i], testing_embeddings[j]])  # Concatenate embeddings\n",
    "        X_Test.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        y_test.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_Test = np.array(X_Test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 621\n",
      "Original pairs: 1225\n"
     ]
    }
   ],
   "source": [
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(testing_embeddings[i])\n",
    "                vec2_jit = jitter_embedding(testing_embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([testing_embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, testing_embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X_Test)}\")\n",
    "# Combine original and augmented data\n",
    "X_Test_combined = np.vstack([X_Test, np.array(augmented_pairs)])\n",
    "y_Test_combined = np.concatenate([y_test, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_combined)\n",
    "X_test_scaled = scaler.transform(X_Test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='f1',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"‚úîÔ∏è Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"‚úîÔ∏è Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "# model.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nüìä Classification Report (Test Set):\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Base estimator\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Parameter grid around your new ‚Äúbest‚Äù run\n",
    "param_grid = {\n",
    "    'n_estimators':       [150, 200, 250],\n",
    "    'learning_rate':      [0.01, 0.03, 0.05],\n",
    "    'max_depth':          [4, 6, 8],\n",
    "    'min_child_weight':   [1, 3, 5],\n",
    "    'subsample':          [0.7, 0.9, 1.0],\n",
    "    'colsample_bytree':   [0.6, 0.8, 1.0],\n",
    "    'gamma':              [0.0, 0.1, 0.2],\n",
    "    'reg_alpha':          [0.0, 0.2, 0.4],\n",
    "    'reg_lambda':         [0.8, 1.0, 1.2],\n",
    "    # ratio = (# negative samples) / (# positive samples)\n",
    "    'scale_pos_weight':   [1, (len(y_combined) - sum(y_combined)) / sum(y_combined)]\n",
    "}\n",
    "\n",
    "# Make a scorer that focuses on recall for label=1\n",
    "recall_pos1 = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# 3. Grid Search (optimize for accuracy; swap scoring to 'recall' if you still want to boost class-1 recall)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator   = xgb_base,\n",
    "    param_grid  = param_grid,\n",
    "    scoring     = 'recall_pos1',\n",
    "    cv          = 2,\n",
    "    n_jobs      = -1,\n",
    "    verbose     = 1,\n",
    "    refit       = True\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Run the search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# 5. Best params & CV score\n",
    "print(\"üèÜ Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# 6. Test‚Äêset evaluation\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "print(\"üìä Classification Report on TEST:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Top results saved to 'grid_search_top_results.txt'\n"
     ]
    }
   ],
   "source": [
    "# Convert cv_results_ to DataFrame\n",
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort by validation recall (class 1)\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Select useful columns\n",
    "display_df = cv_results_df_sorted[[\n",
    "    'mean_train_score', 'mean_test_score', 'rank_test_score', 'params'\n",
    "]].rename(columns={\n",
    "    'mean_train_score': 'Train Recall',\n",
    "    'mean_test_score': 'Val Recall',\n",
    "    'rank_test_score': 'Rank'\n",
    "})\n",
    "\n",
    "# Format top 10 results\n",
    "top_results_text = display_df.head(10).to_string(index=False)\n",
    "\n",
    "# Save to text file\n",
    "with open(\"grid_search_top_results.txt\", \"w\") as f:\n",
    "    f.write(\"üìã Top 10 Grid Search Results by Validation Recall (class=1):\\n\\n\")\n",
    "    f.write(top_results_text)\n",
    "\n",
    "print(\"‚úÖ Top results saved to 'grid_search_top_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä XGBoost - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      1156\n",
      "           1       0.76      0.87      0.81       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.84      0.85      0.84      1846\n",
      "weighted avg       0.86      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,             # Slightly reduced to prevent overfitting\n",
    "    learning_rate=0.05,           # Slower learning for better generalization\n",
    "    max_depth=6,                  # Shallower trees reduce overfitting\n",
    "    min_child_weight=5,           # Prevents learning from overly specific patterns\n",
    "    subsample=0.8,                # Keeps generalization\n",
    "    colsample_bytree=0.8,         # Same: helps with variance\n",
    "    gamma=0.0,                    # Adds split regularization\n",
    "    reg_alpha=0.2,                # L1 regularization\n",
    "    reg_lambda=0.8,               # L2 regularization\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (len(y_combined) - sum(y_combined)) / sum(y_combined)\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_combined)\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nüìä XGBoost - Classification Report:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonized Dataset1 columns: ['strike', 'last', 'mark', 'bid', 'bid_size', 'ask', 'ask_size', 'volume']\n",
      "Harmonized Dataset2 columns: ['open_interest', 'implied_volatility', 'delta', 'gamma', 'theta', 'vega', 'rho']\n",
      "\n",
      "Compatibility Score: 0.93\n",
      "Datasets are COMPATIBLE\n",
      "\n",
      "üîó Compatible Column Pairs:\n",
      "- strike ‚ÜîÔ∏è open_interest\n",
      "- strike ‚ÜîÔ∏è gamma\n",
      "- strike ‚ÜîÔ∏è theta\n",
      "- strike ‚ÜîÔ∏è vega\n",
      "- last ‚ÜîÔ∏è gamma\n",
      "- last ‚ÜîÔ∏è theta\n",
      "- last ‚ÜîÔ∏è vega\n",
      "- bid ‚ÜîÔ∏è gamma\n",
      "- bid ‚ÜîÔ∏è theta\n",
      "- bid_size ‚ÜîÔ∏è open_interest\n",
      "- bid_size ‚ÜîÔ∏è implied_volatility\n",
      "- bid_size ‚ÜîÔ∏è gamma\n",
      "- bid_size ‚ÜîÔ∏è theta\n",
      "- bid_size ‚ÜîÔ∏è rho\n",
      "- ask ‚ÜîÔ∏è gamma\n",
      "- ask ‚ÜîÔ∏è theta\n",
      "- ask_size ‚ÜîÔ∏è open_interest\n",
      "- volume ‚ÜîÔ∏è open_interest\n",
      "- volume ‚ÜîÔ∏è delta\n",
      "- volume ‚ÜîÔ∏è gamma\n",
      "- volume ‚ÜîÔ∏è theta\n",
      "- volume ‚ÜîÔ∏è vega\n",
      "\n",
      "‚úÖ Embeddings and compatibility data saved to 'compatibility_analysis.pkl'\n"
     ]
    }
   ],
   "source": [
    "xgb_model = pickle.load(open('model.pkl', 'rb'))\n",
    "scaler = StandardScaler()\n",
    "# ‚îÄ‚îÄ Your helper functions ‚îÄ‚îÄ\n",
    "\n",
    "def preprocess_keyword(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a column name for comparison.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        s.lower()\n",
    "         .strip()\n",
    "         .replace(\"_\", \" \")\n",
    "         .replace(\"-\", \" \")\n",
    "         .replace(\"#\", \" \")\n",
    "         .replace(\"@\", \" \")\n",
    "    )\n",
    "\n",
    "def extract_columns(csv_path: str):\n",
    "    \"\"\"\n",
    "    Read just the header row and return \n",
    "    (preprocessed list, raw list) of column names.\n",
    "    \"\"\"\n",
    "    raw = list(pd.read_csv(csv_path, nrows=0).columns)\n",
    "    pre = [preprocess_keyword(c) for c in raw]\n",
    "    return pre, raw\n",
    "\n",
    "def build_subset_mapping(src_pre, tgt_pre):\n",
    "    \"\"\"\n",
    "    Map each src_pre ‚Üí the longest tgt_pre that strictly contains it.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for s in src_pre:\n",
    "        matches = [t for t in tgt_pre if s != t and s in t]\n",
    "        if matches:\n",
    "            mapping[s] = max(matches, key=len)\n",
    "    return mapping\n",
    "\n",
    "# ‚îÄ‚îÄ Paths & header extraction ‚îÄ‚îÄ\n",
    "\n",
    "path1 = '../../Datasets/Ingestor_Datasets/Dataset1.csv'\n",
    "path2 = '../../Datasets/Ingestor_Datasets/Dataset2.csv'\n",
    "\n",
    "cols1_pp, cols1_raw = extract_columns(path1)\n",
    "cols2_pp, cols2_raw = extract_columns(path2)\n",
    "\n",
    "# ‚îÄ‚îÄ Subset‚Äêbased renaming ‚îÄ‚îÄ\n",
    "\n",
    "map1_pp = build_subset_mapping(cols1_pp, cols2_pp)  # A‚ÜíB\n",
    "map2_pp = build_subset_mapping(cols2_pp, cols1_pp)  # B‚ÜíA\n",
    "\n",
    "pp_to_raw1 = dict(zip(cols1_pp, cols1_raw))\n",
    "pp_to_raw2 = dict(zip(cols2_pp, cols2_raw))\n",
    "\n",
    "rename1 = {\n",
    "    pp_to_raw1[src]: pp_to_raw2[tgt]\n",
    "    for src, tgt in map1_pp.items()\n",
    "    if src in pp_to_raw1 and tgt in pp_to_raw2\n",
    "}\n",
    "rename2 = {\n",
    "    pp_to_raw2[src]: pp_to_raw1[tgt]\n",
    "    for src, tgt in map2_pp.items()\n",
    "    if src in pp_to_raw2 and tgt in pp_to_raw1\n",
    "}\n",
    "\n",
    "cols1_h = [rename1.get(c, c) for c in cols1_raw]\n",
    "cols2_h = [rename2.get(c, c) for c in cols2_raw]\n",
    "\n",
    "print(\"Harmonized Dataset1 columns:\", cols1_h)\n",
    "print(\"Harmonized Dataset2 columns:\", cols2_h)\n",
    "\n",
    "# ‚îÄ‚îÄ Embedding & matching pipeline ‚îÄ‚îÄ\n",
    "\n",
    "# 1) Generate embeddings (one arg!)\n",
    "embeddings_1 = generate_embeddings(cols1_h, tokenizer, model, device)\n",
    "embeddings_2 = generate_embeddings(cols2_h, tokenizer, model, device)\n",
    "\n",
    "# 2) Build pairwise embeddings + track pairs\n",
    "pair_embeddings = []\n",
    "column_pairs   = []\n",
    "for i, e1 in enumerate(embeddings_1):\n",
    "    for j, e2 in enumerate(embeddings_2):\n",
    "        pair_embeddings.append(np.concatenate([e1, e2]))\n",
    "        column_pairs.append((cols1_h[i], cols2_h[j]))\n",
    "pair_embeddings = np.vstack(pair_embeddings)\n",
    "\n",
    "# 3) Scale ‚Äî **fit** then transform\n",
    "pair_scaled = scaler.fit_transform(pair_embeddings)\n",
    "# If you already have a saved, pre-fitted scaler:\n",
    "# scaler = load(\"scaler.joblib\")\n",
    "# pair_scaled = scaler.transform(pair_embeddings)\n",
    "\n",
    "# 4) Predict\n",
    "preds = xgb_model.predict(pair_scaled)\n",
    "\n",
    "# 5) Extract compatible pairs\n",
    "compatible_pairs = [\n",
    "    (a, b)\n",
    "    for (a, b), p in zip(column_pairs, preds)\n",
    "    if p == 1\n",
    "]\n",
    "\n",
    "# 6) Compute coverage & harmonic‚Äêmean score\n",
    "matched_A = {a for a, _ in compatible_pairs}\n",
    "matched_B = {b for _, b in compatible_pairs}\n",
    "\n",
    "coverage_A = len(matched_A) / len(cols1_h)\n",
    "coverage_B = len(matched_B) / len(cols2_h)\n",
    "compatibility_score = (hmean([coverage_A, coverage_B])\n",
    "                       if coverage_A and coverage_B else 0.0)\n",
    "\n",
    "# 7) Report\n",
    "print(f\"\\nCompatibility Score: {compatibility_score:.2f}\")\n",
    "print(\"Datasets are COMPATIBLE\" if compatibility_score >= 0.7 \n",
    "      else \"Datasets are NOT compatible\")\n",
    "print(\"\\nüîó Compatible Column Pairs:\")\n",
    "if compatible_pairs:\n",
    "    for a, b in compatible_pairs:\n",
    "        print(f\"- {a} ‚ÜîÔ∏è {b}\")\n",
    "else:\n",
    "    print(\"No compatible columns found.\")\n",
    "\n",
    "output_data = {\n",
    "    'embeddings_1': embeddings_1,         \n",
    "    'embeddings_2': embeddings_2,          \n",
    "    'cols1_h': cols1_h,                    \n",
    "    'cols2_h': cols2_h,                    \n",
    "    'compatibility_score': compatibility_score \n",
    "}\n",
    "\n",
    "# Save to a pickle file\n",
    "with open('compatibility_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)\n",
    "\n",
    "print(\"\\n‚úÖ Embeddings and compatibility data saved to 'compatibility_analysis.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
