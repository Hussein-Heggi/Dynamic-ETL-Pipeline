{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "import pickle\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Features = [\n",
    "    \"ticker\", \"company_name\", \"sector\", \"industry\", \"market_cap\",\n",
    "    \"price\", \"open\", \"close\", \"high\", \"low\",\n",
    "    \"volume\", \"adj_close\", \"dividend\", \"dividend_yield\", \"pe_ratio\",\n",
    "    \"eps\", \"beta\", \"52_week_high\", \"52_week_low\", \"shares_outstanding\",\n",
    "    \"float\", \"average_volume\", \"market\", \"exchange\", \"isin\",\n",
    "    \"cusip\", \"country\", \"currency\", \"ipo_date\", \"earnings_date\",\n",
    "    \"revenue\", \"cost_of_goods_sold\", \"gross_profit\", \"operating_expenses\", \"operating_income\",\n",
    "    \"ebit\", \"ebitda\", \"net_income\", \"income_before_tax\", \"tax_expense\",\n",
    "    \"net_income_applicable_to_common_shares\", \"basic_eps\", \"diluted_eps\", \"total_assets\", \"current_assets\",\n",
    "    \"non_current_assets\", \"total_liabilities\", \"current_liabilities\", \"non_current_liabilities\", \"shareholders_equity\",\n",
    "    \"retained_earnings\", \"cash_and_cash_equivalents\", \"short_term_investments\", \"long_term_investments\", \"inventory\",\n",
    "    \"accounts_receivable\", \"accounts_payable\", \"depreciation\", \"amortization\", \"capital_expenditures\",\n",
    "    \"loan_id\", \"loan_amount\", \"loan_term\", \"interest_rate\", \"installment\",\n",
    "    \"issue_date\", \"loan_status\", \"payment_status\", \"borrower_score\", \"borrower_income\",\n",
    "    \"debt_to_income\", \"employment_length\", \"purpose\", \"home_ownership\", \"delinquency_2yrs\",\n",
    "    \"credit_score\", \"fico_range_low\", \"fico_range_high\", \"revol_util\", \"num_open_credit_lines\",\n",
    "    \"total_credit_lines\", \"public_records\", \"collections_12_mths_ex_med\", \"application_type\", \"verification_status\",\n",
    "    \"bond_id\", \"bond_name\", \"maturity_date\", \"coupon_rate\", \"yield_to_maturity\",\n",
    "    \"face_value\", \"issue_price\", \"current_price\", \"duration\", \"convexity\",\n",
    "    \"credit_rating\", \"issuer\", \"callable\", \"puttable\", \"bond_type\",\n",
    "    \"transaction_id\", \"transaction_date\", \"transaction_amount\", \"transaction_type\", \"merchant_name\",\n",
    "    \"merchant_category\", \"account_id\", \"balance_before\", \"balance_after\", \"location\",\n",
    "    \"crypto_symbol\", \"crypto_name\", \"market_rank\", \"circulating_supply\", \"total_supply\",\n",
    "    \"max_supply\", \"market_dominance\", \"all_time_high\", \"all_time_low\", \"last_updated\",\n",
    "    \"block_time\", \"hashing_algorithm\", \"platform\", \"explorer_url\", \"trading_pairs\",\n",
    "    \"exchange_rate\", \"currency_pair\", \"base_currency\", \"quote_currency\", \"rate_date\",\n",
    "    \"rate_time\", \"daily_change\", \"monthly_change\", \"yearly_change\", \"volume_24h\",\n",
    "    \"investment_id\", \"investment_type\", \"investment_amount\", \"investment_date\", \"current_value\",\n",
    "    \"gain_loss\", \"annual_return\", \"investment_duration\", \"investment_strategy\", \"fund_manager\",\n",
    "    \"fund_id\", \"fund_name\", \"nav\", \"expense_ratio\", \"inception_date\",\n",
    "    \"fund_category\", \"assets_under_management\", \"benchmark_index\", \"turnover_ratio\", \"dividend_distribution\",\n",
    "    \"gdp\", \"inflation_rate\", \"unemployment_rate\", \"federal_funds_rate\", \"consumer_price_index\",\n",
    "    \"producer_price_index\", \"retail_sales\", \"housing_starts\", \"trade_balance\", \"government_debt\",\n",
    "    \"current_account_balance\", \"budget_deficit\", \"foreign_reserves\", \"money_supply\", \"taxpayer_id\",\n",
    "    \"income_bracket\", \"taxable_income\", \"effective_tax_rate\", \"tax_paid\", \"deductions\",\n",
    "    \"credits\", \"filing_status\", \"tax_year\", \"refund_amount\", \"bank_id\",\n",
    "    \"branch_id\", \"account_type\", \"account_open_date\", \"account_balance\", \"interest_earned\",\n",
    "    \"overdraft_limit\", \"minimum_balance\", \"monthly_fee\", \"account_status\", \"user_id\",\n",
    "    \"customer_id\", \"registration_date\", \"last_login\", \"kyc_status\", \"risk_score\",\n",
    "    \"fraud_flag\", \"device_id\", \"ip_address\", \"login_location\", \"portfolio_id\",\n",
    "    \"asset_class\", \"allocation_percentage\", \"benchmark_return\", \"tracking_error\", \"sharpe_ratio\",\n",
    "    \"alpha\", \"beta_coefficient\", \"standard_deviation\", \"max_drawdown\", \"audit_status\", \"accounting_standard\", \"financial_statement_type\", \"reporting_currency\", \"adjustment_reason\",\n",
    "    \"deferred_tax_assets\", \"deferred_tax_liabilities\", \"intangible_assets\", \"goodwill\", \"preferred_equity\",\n",
    "    \"policy_id\", \"policy_holder\", \"premium_amount\", \"coverage_amount\", \"claim_id\",\n",
    "    \"claim_status\", \"underwriting_score\", \"risk_class\", \"loss_ratio\", \"combined_ratio\",\n",
    "    \"order_id\", \"trade_price\", \"trade_volume\", \"order_type\", \"execution_time\",\n",
    "    \"bid_price\", \"ask_price\", \"spread\", \"order_book_depth\", \"trading_halt\",\n",
    "    \"swift_code\", \"iban\", \"routing_number\", \"account_opening_method\", \"branch_location\",\n",
    "    \"atm_withdrawals\", \"wire_transfers\", \"monthly_statements\", \"account_tier\", \"fee_structure\",\n",
    "    \"credit_limit\", \"credit_line_type\", \"charge_off_status\", \"days_past_due\", \"collection_agency\",\n",
    "    \"restructuring_status\", \"forbearance_flag\", \"loan_purpose\", \"collateral_type\", \"repayment_behavior\",\n",
    "    \"employment_rate\", \"labor_force_participation\", \"consumer_confidence_index\", \"housing_index\", \"manufacturing_index\",\n",
    "    \"import_volume\", \"export_volume\", \"interest_payment\", \"sovereign_rating\", \"external_debt\",\n",
    "    \"payment_method\", \"payment_gateway\", \"settlement_status\", \"refund_status\", \"dispute_id\",\n",
    "    \"chargeback_amount\", \"recurring_payment\", \"subscription_id\", \"billing_cycle\", \"invoice_date\",\n",
    "    \"wealth_segment\", \"advisor_id\", \"fee_schedule\", \"client_risk_profile\", \"discretionary_mandate\",\n",
    "    \"goals_based_plan\", \"financial_goal\", \"investment_objective\", \"cash_allocation\", \"equity_allocation\",\n",
    "    \"esg_score\", \"carbon_emission\", \"sustainability_rating\", \"board_diversity\", \"executive_compensation_ratio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Features = [\n",
    "   \"revenue_growth\", \"net_margin\", \"operating_margin\", \"book_value_per_share\", \"enterprise_value\",\n",
    "   \"ev_to_ebitda\", \"price_to_free_cash_flow\", \"fcf_margin\", \"roic\", \"roa\",\n",
    "   \"cash_conversion_cycle\", \"interest_coverage_ratio\", \"days_payable_outstanding\", \"inventory_turnover\", \"quick_ratio\",\n",
    "   \"z_score\", \"altman_z_score\", \"short_interest_ratio\", \"put_call_ratio\", \"analyst_recommendation\",\n",
    "   \"price_target_high\", \"price_target_low\", \"estimate_revision\", \"guidance_change\", \"buyback_yield\",\n",
    "   \"s&p_rating\", \"moody_rating\", \"recovery_rate\", \"default_probability\", \"credit_spread\",\n",
    "   \"real_interest_rate\", \"velocity_of_money\", \"consumer_sentiment_index\", \"labor_cost_index\", \"construction_spending\",\n",
    "   \"crypto_funding_rate\", \"staking_yield\", \"token_burn_rate\", \"dao_votes\", \"mining_difficulty\",\n",
    "   \"digital_wallet_id\", \"transaction_fee\", \"payment_token\", \"subscription_status\", \"auto_renew_flag\",\n",
    "   \"claim_frequency\", \"premium_to_coverage_ratio\", \"lapse_rate\", \"policy_duration\", \"actuarial_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_keyword(keyword):\n",
    "    return keyword.replace(\"_\", \" \").lower()\n",
    "\n",
    "# Preprocess training and testing features\n",
    "processed_keywords = [preprocess_keyword(k) for k in Train_Features]\n",
    "processed_testing_keywords = [preprocess_keyword(k) for k in Test_Features]\n",
    "\n",
    "# Load FinancialBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "# Function to generate embeddings using FinancialBERT\n",
    "def generate_embeddings(keywords, tokenizer, model, device):\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "# Generate embeddings for training and testing data\n",
    "embeddings = generate_embeddings(processed_keywords, tokenizer, model, device)\n",
    "testing_embeddings = generate_embeddings(processed_testing_keywords, tokenizer, model, device)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Perform t-SNE visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "# Annotate each point with its corresponding label\n",
    "for i, label in enumerate(processed_keywords):\n",
    "    plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=7, alpha=0.8)\n",
    "\n",
    "# Add title and grid\n",
    "plt.title(\"t-SNE Visualization of Financial Keyword Embeddings using FinancialBERT\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42, gamma='scale')\n",
    "\n",
    "threshold_candidates = np.arange(0.8, 0.9, 0.01)\n",
    "\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "\n",
    "min_positives_ratio = 0.01\n",
    "max_positives_ratio = 0.10\n",
    "\n",
    "\n",
    "for threshold in threshold_candidates:\n",
    "    X_temp, y_temp = [], []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            vec1, vec2 = embeddings[i], embeddings[j]\n",
    "            cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            label = 1 if cos_sim > threshold else 0\n",
    "            X_temp.append(np.concatenate([vec1, vec2]))\n",
    "            y_temp.append(label)\n",
    "\n",
    "    X_temp, y_temp = np.array(X_temp), np.array(y_temp)\n",
    "    positives_ratio = np.mean(y_temp)\n",
    "\n",
    "    # Skip thresholds with too few or too many positives\n",
    "    if not (min_positives_ratio <= positives_ratio <= max_positives_ratio):\n",
    "        print(f\"⚠️ Skipping threshold {threshold:.3f} due to positive ratio: {positives_ratio:.4f}\")\n",
    "        continue\n",
    "\n",
    "    # Split and scale\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "    X_val_scaled = scaler.transform(X_val_sub)\n",
    "\n",
    "    # Train and evaluate\n",
    "    svm_model.fit(X_train_scaled, y_train_sub)\n",
    "    y_pred_val = svm_model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_sub, y_pred_val, zero_division=0)\n",
    "\n",
    "    print(f\"✅ Threshold: {threshold:.3f} | F1: {f1:.4f} | Positive Ratio: {positives_ratio:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "if best_threshold is not None:\n",
    "    print(f\"\\n🎯 Best Threshold Found: {best_threshold:.3f} with F1-score: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n❌ No valid threshold found within ratio constraints. Try adjusting the limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X = []  # Input features (concatenated embeddings)\n",
    "y = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        combined_features = np.concatenate([embeddings[i], embeddings[j]])  # Concatenate embeddings\n",
    "        X.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        y.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Original pairs: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 29169\n",
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "def jitter_embedding(embedding, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, embedding.shape)\n",
    "    return embedding + noise\n",
    "\n",
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(embeddings[i])\n",
    "                vec2_jit = jitter_embedding(embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X)}\")\n",
    "# Combine original and augmented data\n",
    "X_combined = np.vstack([X, np.array(augmented_pairs)])\n",
    "y_combined = np.concatenate([y, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X_Test = []  # Input features (concatenated embeddings)\n",
    "y_test = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        combined_features = np.concatenate([testing_embeddings[i], testing_embeddings[j]])  # Concatenate embeddings\n",
    "        X_Test.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        y_test.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_Test = np.array(X_Test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 621\n",
      "Original pairs: 1225\n"
     ]
    }
   ],
   "source": [
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(testing_embeddings[i])\n",
    "                vec2_jit = jitter_embedding(testing_embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([testing_embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, testing_embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X_Test)}\")\n",
    "# Combine original and augmented data\n",
    "X_Test_combined = np.vstack([X_Test, np.array(augmented_pairs)])\n",
    "y_Test_combined = np.concatenate([y_test, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_combined)\n",
    "X_test_scaled = scaler.transform(X_Test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='f1',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"✔️ Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n✅ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 90 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=59.2min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=60.0min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=60.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=60.4min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=60.7min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=60.8min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=60.9min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=71.5min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=71.6min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.7min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=72.9min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=73.0min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=73.2min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=74.0min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=81.3min\n",
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=85.9min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=86.4min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=88.3min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=89.9min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=59.6min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=60.5min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=61.4min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=62.3min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=61.4min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=63.0min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=61.9min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=75.7min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=74.7min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=89.8min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=91.2min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=77.0min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=90.8min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=90.7min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=90.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=75.4min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=74.4min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=91.7min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=92.1min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=92.1min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=94.4min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=61.3min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.9min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=48.7min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=49.3min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=57.7min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=74.4min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=45.1min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=61.7min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=76.5min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=62.0min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=94.0min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=54.6min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=75.3min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=92.9min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=45.4min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=94.5min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=80.2min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=55.1min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=93.7min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=45.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=55.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=45.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=56.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=55.6min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=46.0min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=56.1min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=82.0min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=97.5min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=56.7min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=45.9min\n",
      "[CV] END .............C=3, degree=3, gamma=auto, kernel=poly; total time=81.7min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=99.2min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=45.3min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=56.8min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=100.0min\n",
      "[CV] END ............C=4, degree=3, gamma=scale, kernel=poly; total time=82.2min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=101.7min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=4, degree=3, gamma=auto, kernel=poly; total time=82.4min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=45.2min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=57.2min\n",
      "[CV] END ...........C=4, degree=3, gamma=scale, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=3, degree=3, gamma=auto, kernel=poly; total time=123.5min\n",
      "[CV] END ............C=4, degree=3, gamma=auto, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=5, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=44.7min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=5, degree=3, gamma=auto, kernel=poly; total time=82.6min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=57.9min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ...........C=5, degree=3, gamma=scale, kernel=poly; total time=104.9min\n",
      "[CV] END ............C=6, degree=3, gamma=scale, kernel=poly; total time=82.3min\n",
      "[CV] END ............C=5, degree=3, gamma=auto, kernel=poly; total time=105.8min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=45.1min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=57.7min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=56.6min\n",
      "[CV] END ...........C=6, degree=3, gamma=scale, kernel=poly; total time=105.4min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=44.4min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=100.2min\n",
      "[CV] END .............C=7, degree=3, gamma=auto, kernel=poly; total time=81.9min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=44.6min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=127.7min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=100.3min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ............C=8, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END .............C=8, degree=3, gamma=auto, kernel=poly; total time=81.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=44.0min\n",
      "[CV] END ............C=7, degree=3, gamma=auto, kernel=poly; total time=105.1min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=58.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=58.5min\n",
      "[CV] END ...........C=8, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=44.1min\n",
      "[CV] END ............C=8, degree=3, gamma=auto, kernel=poly; total time=106.3min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=9, degree=3, gamma=scale, kernel=poly; total time=98.7min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=57.3min\n",
      "[CV] END ...........C=9, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END .............C=9, degree=3, gamma=auto, kernel=poly; total time=99.6min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=43.8min\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=81.3min\n",
      "[CV] END ............C=9, degree=3, gamma=auto, kernel=poly; total time=107.1min\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=81.2min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ..........C=10, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=10, degree=3, gamma=auto, kernel=poly; total time=106.1min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=3, gamma=scale, kernel=poly; total time=80.8min\n",
      "[CV] END ............C=11, degree=3, gamma=auto, kernel=poly; total time=81.4min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=43.7min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=43.7min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=58.2min\n",
      "[CV] END ...........C=12, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=72.7min\n",
      "[CV] END ..........C=11, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=44.1min\n",
      "[CV] END ...........C=11, degree=3, gamma=auto, kernel=poly; total time=107.6min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=80.8min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=55.6min\n",
      "[CV] END ..........C=12, degree=3, gamma=scale, kernel=poly; total time=105.2min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=36.5min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=34.5min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=71.5min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=97.9min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=76.7min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=87.6min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=52.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=71.4min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=85.6min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=58.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=78.2min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=41.9min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=65.8min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=46.9min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=41.0min\n",
      "✔️ Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "✔️ Params: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "✔️ Params: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8677 (+/- 0.1252)\n",
      "✔️ Params: {'C': 2, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8675 (+/- 0.1254)\n",
      "✔️ Params: {'C': 3, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8604 (+/- 0.1344)\n",
      "✔️ Params: {'C': 3, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8603 (+/- 0.1345)\n",
      "✔️ Params: {'C': 4, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "✔️ Params: {'C': 4, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "✔️ Params: {'C': 5, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8553 (+/- 0.1431)\n",
      "✔️ Params: {'C': 5, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8552 (+/- 0.1431)\n",
      "✔️ Params: {'C': 6, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1459)\n",
      "✔️ Params: {'C': 6, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1458)\n",
      "✔️ Params: {'C': 7, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "✔️ Params: {'C': 7, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "✔️ Params: {'C': 8, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8495 (+/- 0.1495)\n",
      "✔️ Params: {'C': 8, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8493 (+/- 0.1496)\n",
      "✔️ Params: {'C': 9, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8484 (+/- 0.1507)\n",
      "✔️ Params: {'C': 9, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8485 (+/- 0.1506)\n",
      "✔️ Params: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "✔️ Params: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "✔️ Params: {'C': 11, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8478 (+/- 0.1522)\n",
      "✔️ Params: {'C': 11, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8477 (+/- 0.1521)\n",
      "✔️ Params: {'C': 12, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "✔️ Params: {'C': 12, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "✔️ Params: {'C': 13, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 13, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 14, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8472 (+/- 0.1528)\n",
      "✔️ Params: {'C': 14, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 15, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.9056 (+/- 0.0844)\n",
      "✔️ Params: {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.9055 (+/- 0.0846)\n",
      "✔️ Params: {'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1310)\n",
      "✔️ Params: {'C': 1, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1310)\n",
      "✔️ Params: {'C': 2, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8812 (+/- 0.1123)\n",
      "✔️ Params: {'C': 2, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8811 (+/- 0.1124)\n",
      "✔️ Params: {'C': 2, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8325 (+/- 0.1591)\n",
      "✔️ Params: {'C': 2, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8323 (+/- 0.1593)\n",
      "✔️ Params: {'C': 3, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1257)\n",
      "✔️ Params: {'C': 3, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1258)\n",
      "✔️ Params: {'C': 3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8220 (+/- 0.1742)\n",
      "✔️ Params: {'C': 3, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8218 (+/- 0.1743)\n",
      "✔️ Params: {'C': 4, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1320)\n",
      "✔️ Params: {'C': 4, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1321)\n",
      "✔️ Params: {'C': 4, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1809)\n",
      "✔️ Params: {'C': 4, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1810)\n",
      "✔️ Params: {'C': 5, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8608 (+/- 0.1355)\n",
      "✔️ Params: {'C': 5, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8607 (+/- 0.1356)\n",
      "✔️ Params: {'C': 5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8122 (+/- 0.1859)\n",
      "✔️ Params: {'C': 5, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8121 (+/- 0.1859)\n",
      "✔️ Params: {'C': 6, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "✔️ Params: {'C': 6, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "✔️ Params: {'C': 6, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8093 (+/- 0.1890)\n",
      "✔️ Params: {'C': 6, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8092 (+/- 0.1892)\n",
      "✔️ Params: {'C': 7, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8572 (+/- 0.1408)\n",
      "✔️ Params: {'C': 7, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8571 (+/- 0.1408)\n",
      "✔️ Params: {'C': 7, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1915)\n",
      "✔️ Params: {'C': 7, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1914)\n",
      "✔️ Params: {'C': 8, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "✔️ Params: {'C': 8, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "✔️ Params: {'C': 8, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "✔️ Params: {'C': 8, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "✔️ Params: {'C': 9, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 9, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1429)\n",
      "✔️ Params: {'C': 9, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8069 (+/- 0.1926)\n",
      "✔️ Params: {'C': 9, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8068 (+/- 0.1926)\n",
      "✔️ Params: {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "✔️ Params: {'C': 10, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "✔️ Params: {'C': 11, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "✔️ Params: {'C': 11, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "✔️ Params: {'C': 11, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "✔️ Params: {'C': 11, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "✔️ Params: {'C': 12, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "✔️ Params: {'C': 12, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "✔️ Params: {'C': 12, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "✔️ Params: {'C': 12, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 13, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1438)\n",
      "✔️ Params: {'C': 13, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1437)\n",
      "✔️ Params: {'C': 13, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 13, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "✔️ Params: {'C': 14, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "✔️ Params: {'C': 14, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "✔️ Params: {'C': 14, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 14, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 15, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8550 (+/- 0.1444)\n",
      "✔️ Params: {'C': 15, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8551 (+/- 0.1444)\n",
      "✔️ Params: {'C': 15, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "✔️ Params: {'C': 15, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "\n",
      "✅ Best Parameters: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"✔️ Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n✅ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      1156\n",
      "           1       0.88      0.69      0.78       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.86      0.82      0.83      1846\n",
      "weighted avg       0.85      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "# model.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n📊 Classification Report (Test Set):\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(csv_path):\n",
    "    raw_columns = list(pd.read_csv(csv_path, nrows=0).columns)\n",
    "    return [preprocess_keyword(col) for col in raw_columns]\n",
    "\n",
    "# Step 1: Load and preprocess columns\n",
    "columns_1 = extract_columns(\"Datasets/Banking/dataset1/loan_data.csv\")       # Dataset A\n",
    "columns_2 = extract_columns(\"Datasets/Loans/dataset2/loan_data.csv\")  # Dataset B (note: different folder!)\n",
    "\n",
    "# Step 2: Generate embeddings (assuming your function works correctly)\n",
    "embeddings_1 = generate_embeddings(columns_1, tokenizer, model, device)\n",
    "embeddings_2 = generate_embeddings(columns_2, tokenizer, model, device)\n",
    "\n",
    "# Step 3: Create pair embeddings between each column from dataset 1 and dataset 2\n",
    "Dataset_test = []\n",
    "column_pairs = []\n",
    "\n",
    "for i, emb1 in enumerate(embeddings_1):\n",
    "    for j, emb2 in enumerate(embeddings_2):\n",
    "        pair_embedding = np.concatenate([emb1, emb2])\n",
    "        Dataset_test.append(pair_embedding)\n",
    "        column_pairs.append((columns_1[i], columns_2[j]))\n",
    "\n",
    "Dataset_test = np.array(Dataset_test)\n",
    "\n",
    "# Step 4: Predict with trained model\n",
    "Dataset_test_scaled = scaler.transform(Dataset_test)\n",
    "predictions = svm_model.predict(Dataset_test_scaled)\n",
    "\n",
    "# Step 5: Extract compatible pairs\n",
    "compatible_pairs = [(col1, col2) for (col1, col2), pred in zip(column_pairs, predictions) if pred == 1]\n",
    "\n",
    "# Step 6: Compatibility score based on bidirectional coverage\n",
    "matched_A = set(col1 for col1, _ in compatible_pairs)\n",
    "matched_B = set(col2 for _, col2 in compatible_pairs)\n",
    "\n",
    "coverage_A = len(matched_A) / len(columns_1)\n",
    "coverage_B = len(matched_B) / len(columns_2)\n",
    "\n",
    "# Use harmonic mean for stricter measure (penalizes one-sided matching)\n",
    "if coverage_A > 0 and coverage_B > 0:\n",
    "    compatibility_score = hmean([coverage_A, coverage_B])\n",
    "else:\n",
    "    compatibility_score = 0.0\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"\\nCompatibility Score: {compatibility_score:.2f}\")\n",
    "print(\"Datasets are COMPATIBLE\" if compatibility_score >= 0.7 else \"Datasets are NOT compatible\")\n",
    "\n",
    "print(\"\\n🔗 Compatible Column Pairs:\")\n",
    "if compatible_pairs:\n",
    "    for col1, col2 in compatible_pairs:\n",
    "        print(f\"- {col1}  ↔️  {col2}\")\n",
    "else:\n",
    "    print(\"No compatible columns found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
