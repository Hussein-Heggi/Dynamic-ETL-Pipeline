{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, make_scorer, recall_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Features = [\n",
    "    \"ticker\", \"company_name\", \"sector\", \"industry\", \"market_cap\",\n",
    "    \"price\", \"open\", \"close\", \"high\", \"low\",\n",
    "    \"volume\", \"adj_close\", \"dividend\", \"dividend_yield\", \"pe_ratio\",\n",
    "    \"eps\", \"beta\", \"52_week_high\", \"52_week_low\", \"shares_outstanding\",\n",
    "    \"float\", \"average_volume\", \"market\", \"exchange\", \"isin\",\n",
    "    \"cusip\", \"country\", \"currency\", \"ipo_date\", \"earnings_date\",\n",
    "    \"revenue\", \"cost_of_goods_sold\", \"gross_profit\", \"operating_expenses\", \"operating_income\",\n",
    "    \"ebit\", \"ebitda\", \"net_income\", \"income_before_tax\", \"tax_expense\",\n",
    "    \"net_income_applicable_to_common_shares\", \"basic_eps\", \"diluted_eps\", \"total_assets\", \"current_assets\",\n",
    "    \"non_current_assets\", \"total_liabilities\", \"current_liabilities\", \"non_current_liabilities\", \"shareholders_equity\",\n",
    "    \"retained_earnings\", \"cash_and_cash_equivalents\", \"short_term_investments\", \"long_term_investments\", \"inventory\",\n",
    "    \"accounts_receivable\", \"accounts_payable\", \"depreciation\", \"amortization\", \"capital_expenditures\",\n",
    "    \"loan_id\", \"loan_amount\", \"loan_term\", \"interest_rate\", \"installment\",\n",
    "    \"issue_date\", \"loan_status\", \"payment_status\", \"borrower_score\", \"borrower_income\",\n",
    "    \"debt_to_income\", \"employment_length\", \"purpose\", \"home_ownership\", \"delinquency_2yrs\",\n",
    "    \"credit_score\", \"fico_range_low\", \"fico_range_high\", \"revol_util\", \"num_open_credit_lines\",\n",
    "    \"total_credit_lines\", \"public_records\", \"collections_12_mths_ex_med\", \"application_type\", \"verification_status\",\n",
    "    \"bond_id\", \"bond_name\", \"maturity_date\", \"coupon_rate\", \"yield_to_maturity\",\n",
    "    \"face_value\", \"issue_price\", \"current_price\", \"duration\", \"convexity\",\n",
    "    \"credit_rating\", \"issuer\", \"callable\", \"puttable\", \"bond_type\",\n",
    "    \"transaction_id\", \"transaction_date\", \"transaction_amount\", \"transaction_type\", \"merchant_name\",\n",
    "    \"merchant_category\", \"account_id\", \"balance_before\", \"balance_after\", \"location\",\n",
    "    \"crypto_symbol\", \"crypto_name\", \"market_rank\", \"circulating_supply\", \"total_supply\",\n",
    "    \"max_supply\", \"market_dominance\", \"all_time_high\", \"all_time_low\", \"last_updated\",\n",
    "    \"block_time\", \"hashing_algorithm\", \"platform\", \"explorer_url\", \"trading_pairs\",\n",
    "    \"exchange_rate\", \"currency_pair\", \"base_currency\", \"quote_currency\", \"rate_date\",\n",
    "    \"rate_time\", \"daily_change\", \"monthly_change\", \"yearly_change\", \"volume_24h\",\n",
    "    \"investment_id\", \"investment_type\", \"investment_amount\", \"investment_date\", \"current_value\",\n",
    "    \"gain_loss\", \"annual_return\", \"investment_duration\", \"investment_strategy\", \"fund_manager\",\n",
    "    \"fund_id\", \"fund_name\", \"nav\", \"expense_ratio\", \"inception_date\",\n",
    "    \"fund_category\", \"assets_under_management\", \"benchmark_index\", \"turnover_ratio\", \"dividend_distribution\",\n",
    "    \"gdp\", \"inflation_rate\", \"unemployment_rate\", \"federal_funds_rate\", \"consumer_price_index\",\n",
    "    \"producer_price_index\", \"retail_sales\", \"housing_starts\", \"trade_balance\", \"government_debt\",\n",
    "    \"current_account_balance\", \"budget_deficit\", \"foreign_reserves\", \"money_supply\", \"taxpayer_id\",\n",
    "    \"income_bracket\", \"taxable_income\", \"effective_tax_rate\", \"tax_paid\", \"deductions\",\n",
    "    \"credits\", \"filing_status\", \"tax_year\", \"refund_amount\", \"bank_id\",\n",
    "    \"branch_id\", \"account_type\", \"account_open_date\", \"account_balance\", \"interest_earned\",\n",
    "    \"overdraft_limit\", \"minimum_balance\", \"monthly_fee\", \"account_status\", \"user_id\",\n",
    "    \"customer_id\", \"registration_date\", \"last_login\", \"kyc_status\", \"risk_score\",\n",
    "    \"fraud_flag\", \"device_id\", \"ip_address\", \"login_location\", \"portfolio_id\",\n",
    "    \"asset_class\", \"allocation_percentage\", \"benchmark_return\", \"tracking_error\", \"sharpe_ratio\",\n",
    "    \"alpha\", \"beta_coefficient\", \"standard_deviation\", \"max_drawdown\", \"audit_status\", \"accounting_standard\", \"financial_statement_type\", \"reporting_currency\", \"adjustment_reason\",\n",
    "    \"deferred_tax_assets\", \"deferred_tax_liabilities\", \"intangible_assets\", \"goodwill\", \"preferred_equity\",\n",
    "    \"policy_id\", \"policy_holder\", \"premium_amount\", \"coverage_amount\", \"claim_id\",\n",
    "    \"claim_status\", \"underwriting_score\", \"risk_class\", \"loss_ratio\", \"combined_ratio\",\n",
    "    \"order_id\", \"trade_price\", \"trade_volume\", \"order_type\", \"execution_time\",\n",
    "    \"bid_price\", \"ask_price\", \"spread\", \"order_book_depth\", \"trading_halt\",\n",
    "    \"swift_code\", \"iban\", \"routing_number\", \"account_opening_method\", \"branch_location\",\n",
    "    \"atm_withdrawals\", \"wire_transfers\", \"monthly_statements\", \"account_tier\", \"fee_structure\",\n",
    "    \"credit_limit\", \"credit_line_type\", \"charge_off_status\", \"days_past_due\", \"collection_agency\",\n",
    "    \"restructuring_status\", \"forbearance_flag\", \"loan_purpose\", \"collateral_type\", \"repayment_behavior\",\n",
    "    \"employment_rate\", \"labor_force_participation\", \"consumer_confidence_index\", \"housing_index\", \"manufacturing_index\",\n",
    "    \"import_volume\", \"export_volume\", \"interest_payment\", \"sovereign_rating\", \"external_debt\",\n",
    "    \"payment_method\", \"payment_gateway\", \"settlement_status\", \"refund_status\", \"dispute_id\",\n",
    "    \"chargeback_amount\", \"recurring_payment\", \"subscription_id\", \"billing_cycle\", \"invoice_date\",\n",
    "    \"wealth_segment\", \"advisor_id\", \"fee_schedule\", \"client_risk_profile\", \"discretionary_mandate\",\n",
    "    \"goals_based_plan\", \"financial_goal\", \"investment_objective\", \"cash_allocation\", \"equity_allocation\",\n",
    "    \"esg_score\", \"carbon_emission\", \"sustainability_rating\", \"board_diversity\", \"executive_compensation_ratio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Features = [\n",
    "   \"revenue_growth\", \"net_margin\", \"operating_margin\", \"book_value_per_share\", \"enterprise_value\",\n",
    "   \"ev_to_ebitda\", \"price_to_free_cash_flow\", \"fcf_margin\", \"roic\", \"roa\",\n",
    "   \"cash_conversion_cycle\", \"interest_coverage_ratio\", \"days_payable_outstanding\", \"inventory_turnover\", \"quick_ratio\",\n",
    "   \"z_score\", \"altman_z_score\", \"short_interest_ratio\", \"put_call_ratio\", \"analyst_recommendation\",\n",
    "   \"price_target_high\", \"price_target_low\", \"estimate_revision\", \"guidance_change\", \"buyback_yield\",\n",
    "   \"s&p_rating\", \"moody_rating\", \"recovery_rate\", \"default_probability\", \"credit_spread\",\n",
    "   \"real_interest_rate\", \"velocity_of_money\", \"consumer_sentiment_index\", \"labor_cost_index\", \"construction_spending\",\n",
    "   \"crypto_funding_rate\", \"staking_yield\", \"token_burn_rate\", \"dao_votes\", \"mining_difficulty\",\n",
    "   \"digital_wallet_id\", \"transaction_fee\", \"payment_token\", \"subscription_status\", \"auto_renew_flag\",\n",
    "   \"claim_frequency\", \"premium_to_coverage_ratio\", \"lapse_rate\", \"policy_duration\", \"actuarial_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_keyword(keyword):\n",
    "    return keyword.replace(\"_\", \" \").lower()\n",
    "\n",
    "# Preprocess training and testing features\n",
    "processed_keywords = [preprocess_keyword(k) for k in Train_Features]\n",
    "processed_testing_keywords = [preprocess_keyword(k) for k in Test_Features]\n",
    "\n",
    "# Load FinancialBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "# Function to generate embeddings using FinancialBERT\n",
    "def generate_embeddings(keywords, tokenizer, model, device):\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "# Generate embeddings for training and testing data\n",
    "embeddings = generate_embeddings(processed_keywords, tokenizer, model, device)\n",
    "testing_embeddings = generate_embeddings(processed_testing_keywords, tokenizer, model, device)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Perform t-SNE visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "# Annotate each point with its corresponding label\n",
    "for i, label in enumerate(processed_keywords):\n",
    "    plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=7, alpha=0.8)\n",
    "\n",
    "# Add title and grid\n",
    "plt.title(\"t-SNE Visualization of Financial Keyword Embeddings using FinancialBERT\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping threshold 0.800 due to positive ratio: 0.2489\n",
      "⚠️ Skipping threshold 0.810 due to positive ratio: 0.2062\n",
      "⚠️ Skipping threshold 0.820 due to positive ratio: 0.1683\n",
      "⚠️ Skipping threshold 0.830 due to positive ratio: 0.1335\n",
      "⚠️ Skipping threshold 0.840 due to positive ratio: 0.1016\n"
     ]
    }
   ],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42, gamma='scale')\n",
    "\n",
    "threshold_candidates = np.arange(0.8, 0.9, 0.01)\n",
    "\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "\n",
    "min_positives_ratio = 0.01\n",
    "max_positives_ratio = 0.10\n",
    "\n",
    "\n",
    "for threshold in threshold_candidates:\n",
    "    X_temp, y_temp = [], []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            vec1, vec2 = embeddings[i], embeddings[j]\n",
    "            cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            label = 1 if cos_sim > threshold else 0\n",
    "            X_temp.append(np.concatenate([vec1, vec2]))\n",
    "            y_temp.append(label)\n",
    "\n",
    "    X_temp, y_temp = np.array(X_temp), np.array(y_temp)\n",
    "    positives_ratio = np.mean(y_temp)\n",
    "\n",
    "    # Skip thresholds with too few or too many positives\n",
    "    if not (min_positives_ratio <= positives_ratio <= max_positives_ratio):\n",
    "        print(f\"⚠️ Skipping threshold {threshold:.3f} due to positive ratio: {positives_ratio:.4f}\")\n",
    "        continue\n",
    "\n",
    "    # Split and scale\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "    X_val_scaled = scaler.transform(X_val_sub)\n",
    "\n",
    "    # Train and evaluate\n",
    "    svm_model.fit(X_train_scaled, y_train_sub)\n",
    "    y_pred_val = svm_model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_sub, y_pred_val, zero_division=0)\n",
    "\n",
    "    print(f\"✅ Threshold: {threshold:.3f} | F1: {f1:.4f} | Positive Ratio: {positives_ratio:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "if best_threshold is not None:\n",
    "    print(f\"\\n🎯 Best Threshold Found: {best_threshold:.3f} with F1-score: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n❌ No valid threshold found within ratio constraints. Try adjusting the limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X = []  # Input features (concatenated embeddings)\n",
    "y = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        combined_features = np.concatenate([embeddings[i], embeddings[j]])  # Concatenate embeddings\n",
    "        X.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        y.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Original pairs: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 29169\n",
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "def jitter_embedding(embedding, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, embedding.shape)\n",
    "    return embedding + noise\n",
    "\n",
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(embeddings[i])\n",
    "                vec2_jit = jitter_embedding(embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X)}\")\n",
    "# Combine original and augmented data\n",
    "X_combined = np.vstack([X, np.array(augmented_pairs)])\n",
    "y_combined = np.concatenate([y, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X_Test = []  # Input features (concatenated embeddings)\n",
    "y_test = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        combined_features = np.concatenate([testing_embeddings[i], testing_embeddings[j]])  # Concatenate embeddings\n",
    "        X_Test.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        y_test.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_Test = np.array(X_Test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 621\n",
      "Original pairs: 1225\n"
     ]
    }
   ],
   "source": [
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(testing_embeddings[i])\n",
    "                vec2_jit = jitter_embedding(testing_embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([testing_embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, testing_embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X_Test)}\")\n",
    "# Combine original and augmented data\n",
    "X_Test_combined = np.vstack([X_Test, np.array(augmented_pairs)])\n",
    "y_Test_combined = np.concatenate([y_test, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_combined)\n",
    "X_test_scaled = scaler.transform(X_Test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='f1',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"✔️ Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n✅ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 90 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=59.2min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=60.0min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=60.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=60.4min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=60.7min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=60.8min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=60.9min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=71.5min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=71.6min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.7min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=72.9min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=73.0min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=73.2min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=74.0min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=81.3min\n",
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=85.9min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=86.4min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=88.3min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=89.9min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=59.6min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=60.5min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=61.4min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=62.3min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=61.4min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=63.0min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=61.9min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=75.7min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=74.7min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=89.8min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=91.2min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=77.0min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=90.8min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=90.7min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=90.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=75.4min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=74.4min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=91.7min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=92.1min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=92.1min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=94.4min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=61.3min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.9min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=48.7min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=49.3min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=57.7min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=74.4min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=45.1min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=61.7min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=76.5min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=62.0min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=94.0min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=54.6min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=75.3min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=92.9min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=45.4min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=94.5min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=80.2min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=55.1min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=93.7min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=45.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=55.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=45.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=56.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=55.6min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=46.0min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=56.1min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=82.0min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=97.5min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=56.7min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=45.9min\n",
      "[CV] END .............C=3, degree=3, gamma=auto, kernel=poly; total time=81.7min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=99.2min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=45.3min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=56.8min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=100.0min\n",
      "[CV] END ............C=4, degree=3, gamma=scale, kernel=poly; total time=82.2min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=101.7min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=4, degree=3, gamma=auto, kernel=poly; total time=82.4min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=45.2min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=57.2min\n",
      "[CV] END ...........C=4, degree=3, gamma=scale, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=3, degree=3, gamma=auto, kernel=poly; total time=123.5min\n",
      "[CV] END ............C=4, degree=3, gamma=auto, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=5, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=44.7min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=5, degree=3, gamma=auto, kernel=poly; total time=82.6min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=57.9min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ...........C=5, degree=3, gamma=scale, kernel=poly; total time=104.9min\n",
      "[CV] END ............C=6, degree=3, gamma=scale, kernel=poly; total time=82.3min\n",
      "[CV] END ............C=5, degree=3, gamma=auto, kernel=poly; total time=105.8min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=45.1min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=57.7min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=56.6min\n",
      "[CV] END ...........C=6, degree=3, gamma=scale, kernel=poly; total time=105.4min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=44.4min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=100.2min\n",
      "[CV] END .............C=7, degree=3, gamma=auto, kernel=poly; total time=81.9min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=44.6min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=127.7min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=100.3min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ............C=8, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END .............C=8, degree=3, gamma=auto, kernel=poly; total time=81.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=44.0min\n",
      "[CV] END ............C=7, degree=3, gamma=auto, kernel=poly; total time=105.1min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=58.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=58.5min\n",
      "[CV] END ...........C=8, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=44.1min\n",
      "[CV] END ............C=8, degree=3, gamma=auto, kernel=poly; total time=106.3min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=9, degree=3, gamma=scale, kernel=poly; total time=98.7min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=57.3min\n",
      "[CV] END ...........C=9, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END .............C=9, degree=3, gamma=auto, kernel=poly; total time=99.6min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=43.8min\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=81.3min\n",
      "[CV] END ............C=9, degree=3, gamma=auto, kernel=poly; total time=107.1min\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=81.2min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ..........C=10, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=10, degree=3, gamma=auto, kernel=poly; total time=106.1min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=3, gamma=scale, kernel=poly; total time=80.8min\n",
      "[CV] END ............C=11, degree=3, gamma=auto, kernel=poly; total time=81.4min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=43.7min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=43.7min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=58.2min\n",
      "[CV] END ...........C=12, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=72.7min\n",
      "[CV] END ..........C=11, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=44.1min\n",
      "[CV] END ...........C=11, degree=3, gamma=auto, kernel=poly; total time=107.6min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=80.8min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=55.6min\n",
      "[CV] END ..........C=12, degree=3, gamma=scale, kernel=poly; total time=105.2min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=36.5min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=34.5min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=71.5min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=97.9min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=76.7min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=87.6min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=52.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=71.4min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=85.6min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=58.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=78.2min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=41.9min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=65.8min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=46.9min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=41.0min\n",
      "✔️ Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "✔️ Params: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "✔️ Params: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8677 (+/- 0.1252)\n",
      "✔️ Params: {'C': 2, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8675 (+/- 0.1254)\n",
      "✔️ Params: {'C': 3, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8604 (+/- 0.1344)\n",
      "✔️ Params: {'C': 3, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8603 (+/- 0.1345)\n",
      "✔️ Params: {'C': 4, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "✔️ Params: {'C': 4, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "✔️ Params: {'C': 5, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8553 (+/- 0.1431)\n",
      "✔️ Params: {'C': 5, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8552 (+/- 0.1431)\n",
      "✔️ Params: {'C': 6, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1459)\n",
      "✔️ Params: {'C': 6, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1458)\n",
      "✔️ Params: {'C': 7, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "✔️ Params: {'C': 7, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "✔️ Params: {'C': 8, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8495 (+/- 0.1495)\n",
      "✔️ Params: {'C': 8, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8493 (+/- 0.1496)\n",
      "✔️ Params: {'C': 9, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8484 (+/- 0.1507)\n",
      "✔️ Params: {'C': 9, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8485 (+/- 0.1506)\n",
      "✔️ Params: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "✔️ Params: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "✔️ Params: {'C': 11, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8478 (+/- 0.1522)\n",
      "✔️ Params: {'C': 11, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8477 (+/- 0.1521)\n",
      "✔️ Params: {'C': 12, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "✔️ Params: {'C': 12, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "✔️ Params: {'C': 13, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 13, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 14, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8472 (+/- 0.1528)\n",
      "✔️ Params: {'C': 14, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 15, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "✔️ Params: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.9056 (+/- 0.0844)\n",
      "✔️ Params: {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.9055 (+/- 0.0846)\n",
      "✔️ Params: {'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1310)\n",
      "✔️ Params: {'C': 1, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1310)\n",
      "✔️ Params: {'C': 2, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8812 (+/- 0.1123)\n",
      "✔️ Params: {'C': 2, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8811 (+/- 0.1124)\n",
      "✔️ Params: {'C': 2, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8325 (+/- 0.1591)\n",
      "✔️ Params: {'C': 2, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8323 (+/- 0.1593)\n",
      "✔️ Params: {'C': 3, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1257)\n",
      "✔️ Params: {'C': 3, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1258)\n",
      "✔️ Params: {'C': 3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8220 (+/- 0.1742)\n",
      "✔️ Params: {'C': 3, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8218 (+/- 0.1743)\n",
      "✔️ Params: {'C': 4, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1320)\n",
      "✔️ Params: {'C': 4, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1321)\n",
      "✔️ Params: {'C': 4, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1809)\n",
      "✔️ Params: {'C': 4, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1810)\n",
      "✔️ Params: {'C': 5, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8608 (+/- 0.1355)\n",
      "✔️ Params: {'C': 5, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8607 (+/- 0.1356)\n",
      "✔️ Params: {'C': 5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8122 (+/- 0.1859)\n",
      "✔️ Params: {'C': 5, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8121 (+/- 0.1859)\n",
      "✔️ Params: {'C': 6, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "✔️ Params: {'C': 6, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "✔️ Params: {'C': 6, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8093 (+/- 0.1890)\n",
      "✔️ Params: {'C': 6, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8092 (+/- 0.1892)\n",
      "✔️ Params: {'C': 7, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8572 (+/- 0.1408)\n",
      "✔️ Params: {'C': 7, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8571 (+/- 0.1408)\n",
      "✔️ Params: {'C': 7, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1915)\n",
      "✔️ Params: {'C': 7, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1914)\n",
      "✔️ Params: {'C': 8, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "✔️ Params: {'C': 8, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "✔️ Params: {'C': 8, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "✔️ Params: {'C': 8, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "✔️ Params: {'C': 9, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 9, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1429)\n",
      "✔️ Params: {'C': 9, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8069 (+/- 0.1926)\n",
      "✔️ Params: {'C': 9, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8068 (+/- 0.1926)\n",
      "✔️ Params: {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "✔️ Params: {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "✔️ Params: {'C': 10, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "✔️ Params: {'C': 11, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "✔️ Params: {'C': 11, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "✔️ Params: {'C': 11, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "✔️ Params: {'C': 11, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "✔️ Params: {'C': 12, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "✔️ Params: {'C': 12, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "✔️ Params: {'C': 12, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "✔️ Params: {'C': 12, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 13, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1438)\n",
      "✔️ Params: {'C': 13, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1437)\n",
      "✔️ Params: {'C': 13, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 13, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "✔️ Params: {'C': 14, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "✔️ Params: {'C': 14, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "✔️ Params: {'C': 14, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 14, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "✔️ Params: {'C': 15, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8550 (+/- 0.1444)\n",
      "✔️ Params: {'C': 15, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8551 (+/- 0.1444)\n",
      "✔️ Params: {'C': 15, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "✔️ Params: {'C': 15, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "\n",
      "✅ Best Parameters: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"✔️ Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n✅ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "# model.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n📊 Classification Report (Test Set):\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1024 candidates, totalling 2048 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.939) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.941) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.942) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.952, test=0.956) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.709) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.780) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.726) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.729) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.731) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.732) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.766) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.786) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.738) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.780) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.944) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.730) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.960) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.956) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.708) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.780) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.768) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.954) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.965) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.963) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.755) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.754) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.801) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.738) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.796) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.754) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.807) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.752) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.942) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.798) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.763) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.957) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.787) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.954) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.764) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.734) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.949) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.966) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.736) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.752) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.801) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.791) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.939) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.937) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.718) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.765) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.736) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.958) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.939) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.714) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.781) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.947) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.767) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.737) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.726) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.782) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.950) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.965) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.749) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.753) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.948) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.809) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.747) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.952) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.746) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.785) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.793) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.760) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.741) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.803) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.956) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.951) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:30:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.962) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.973) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.800) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.830) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.973) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.849) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.827) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.828) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.813) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.834) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.831) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.971) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.810) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.817) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.979) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.973) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.839) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.832) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.828) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.805) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.822) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.842) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.843) total time= 4.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.827) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.860) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:37:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.843) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.829) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.846) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.835) total time= 4.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.830) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.854) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.845) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.829) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.837) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.812) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.801) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.845) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.970) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.807) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.822) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.976) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.838) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.824) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.969) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.972) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.812) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.811) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.839) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.977, test=0.968) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.824) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.968) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.816) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.830) total time= 4.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.820) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.859) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.975) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.974) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.839) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.821) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.838) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.823) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.766) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.960) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.787) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 4.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.959) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.981, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.967) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.814) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.970) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.981) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.835) total time= 4.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.812) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.773) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.982) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.827) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.769) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.849) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.819) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.841) total time= 4.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.970) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.806) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.962) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.779) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.817) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.809) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.959) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.819) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.967) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.795) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.798) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.807) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.840) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.805) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.844) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.968) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.826) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.840) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.973) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.794) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.790) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.801) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.834) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.821) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.960) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.814) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.768) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.822) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.804) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.961) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.960) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.764) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.779) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.962) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.780) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.782) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.824) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.787) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.970) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.968) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.806) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.797) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.976) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.830) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.965) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.968) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.794) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.803) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.840) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.968) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.794) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.975) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.800) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.835) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.829) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.969) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.803) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.973) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.974) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.842) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.805) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.977) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.838) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.857) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.976) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.850) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.845) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.982) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.852) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.874) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.854) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.867) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.872) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.858) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.839) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.843) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.854) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.855) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.869) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.836) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.819) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.856) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.977) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.846) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.836) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.843) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:15:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.863) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:15:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.846) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.870) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.881) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.859) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.844) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.873) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.855) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.860) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.829) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.863) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:18:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:18:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.726) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.854) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.729) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.942) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.765) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.731) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.864) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.939) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.709) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.952, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.766) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:20:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:20:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.732) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.786) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.944) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.730) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.708) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.960) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.956) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.768) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.954) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.755) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.754) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.801) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.963) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.754) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.738) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.952) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.807) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.962) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.755) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.798) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.752) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.949) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.737) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.966) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.801) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.791) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.940) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.787) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.954) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.764) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.718) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.765) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.736) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.714) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.781) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.767) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.937, test=0.947) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.737) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.726) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.782) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.749) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.753) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.809) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.949) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.747) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.747) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.952) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.760) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.739) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.803) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.800) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.973) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.829) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.853) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.827) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.827) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.814) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.833) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.811) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.840) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.971) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.804) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.835) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.825) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.867) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.841) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.830) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.977) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.831) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.852) total time= 4.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.838) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.977) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.808) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.853) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.971) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.804) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.976) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.825) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.816) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.806) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.977, test=0.968) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.824) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.811) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.810) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.986, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.842) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:45:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:45:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.823) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.818) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.974) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.826) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.820) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.976) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.854) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.836) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.821) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.848) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.847) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.961) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.763) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.785) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.814) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.832) total time= 3.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.773) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.769) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.856) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.819) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.806) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.842) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.960, test=0.787) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.958, test=0.781) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.817) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.957) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.813) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.819) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.793) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.970) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.967) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.803) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.835) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.976) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.820) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.804) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.973) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.840) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.826) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.818) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.809) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.841) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.824) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.973) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.834) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.790) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.772) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.819) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.814) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.815) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.960) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.768) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.959, test=0.783) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.813) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.804) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.764) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.962) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.960) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.779) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.961) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.814) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.768) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.783) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.824) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.787) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.968) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.806) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.791) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.976, test=0.830) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.969) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.966) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.794) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.799) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.974) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.976) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.834) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.968) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.825) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.803) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.829) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.836) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.968) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.973) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.842) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.805) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.848) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.986) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.862) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.840) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.831) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.977) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.827) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.984) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.852) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.849) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.851) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.981) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.862) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.860) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.847) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.869) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.872) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.860) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.839) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.990) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.875) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.985) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.862) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.841) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.831) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.853) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.865) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.840) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.833) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.843) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.985) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.826) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.856) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.837) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.828) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.852) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.866) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.872) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.856) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.842) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.874) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.859) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.858) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.871) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.861) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.859) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.938) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.954) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.723) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.770) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.881) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.744) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.939) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.859) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.727) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.786) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.943) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.774) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.783) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.938) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.751) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.752) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.780) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.962) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.769) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.962) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.746) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.796) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.757) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.797) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.952) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.769) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.764) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.802) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.943) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.940) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.773) total time= 2.1min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.799) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.747) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.738) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.954) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.766) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.733) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.741) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.775) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.944) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.936) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.722) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.954) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.742) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.952) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.790) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.747) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.791) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.806) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.764) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.964) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.790) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.965) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.767) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.753) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.962) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.763) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.963) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.804) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.808) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.799) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.976) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.834) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.973) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.826) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.815) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.822) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.972) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.809) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.825) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.807) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.815) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.828) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.822) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.845) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.842) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.978) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.833) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.824) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.849) total time= 4.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.836) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.848) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.828) total time= 4.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.971) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.814) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.833) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.842) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.830) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.848) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.819) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.793) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.839) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.977) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.971) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.812) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.817) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.831) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.823) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.819) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.846) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:44:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:44:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.819) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.841) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.981) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.810) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.844) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.850) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.835) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.771) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.974) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.831) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.766) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.975) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.816) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.799) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.982) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.960, test=0.777) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.767) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.804) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.843) total time= 3.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.959) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.959) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.760) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.805) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.796) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.956) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.767) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.967) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.789) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.977) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.838) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.819) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.967) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.804) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.789) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.833) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.823) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.776) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.826) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.976) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.813) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.787) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.975) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.832) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.755) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.817) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.781) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.818) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.755) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.967) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.822) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.762) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.968) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.811) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.770) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.821) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.801) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.966) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.780) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.976) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.835) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.824) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.965) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.796) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.972) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.779) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.976) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.844) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.821) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.964) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.788) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.834) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.821) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.964) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.791) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.838) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.825) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.981) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.981) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.987) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.836) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.830) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.843) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.840) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.986) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.852) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.826) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.986) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.864) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.857) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.990) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.840) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.876) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.855) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.985) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.854) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.872) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.983) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.989) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.840) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.867) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.987) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.982) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.838) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.983) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.853) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.826) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.987, test=0.982) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.987, test=0.979) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.983) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.834) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.980) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.873) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.845) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.827) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.846) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.856) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.845) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.878) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.868) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.987) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.871) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.848) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.883) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.858) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.986) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.983) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.723) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.848) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.770) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.939) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.744) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.860) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.868) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.731) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.786) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.941) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.734) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.935) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.783) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.751) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.752) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.780) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.950) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.961) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.769) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.745) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.954) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.762) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.757) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.797) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.950) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.769) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.763) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.802) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.787) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.941) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.957) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.938) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.747) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.943) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.799) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.955) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.766) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.954) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.733) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.943) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.938) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.741) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.775) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.721) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.936) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.742) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.953) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.952) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.790) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.951) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.964) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.749) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.791) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.806) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.762) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.752) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.790) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.767) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.751) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.763) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.805) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.972) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.831) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.830) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.974) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.814) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.976) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.823) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.809) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.973) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.810) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.971) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.814) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.826) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.828) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.979) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.821) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.822) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.844) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.848) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.839) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.978) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.841) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.826) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.970) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.816) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.811) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.844) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.831) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.796) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.839) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.978) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.801) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.832) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.818) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.810) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.843) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.833) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.834) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.841) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.817) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.828) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.823) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.846) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.766) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.832) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.816) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.946, test=0.957) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.826) total time= 4.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.767) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.857) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.818) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.810) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.846) total time= 3.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.760) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.805) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.796) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.956) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.767) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.969) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.789) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.836) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.822) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.965) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.797) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.787) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.954, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.828) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.835) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.973) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.776) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.826) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.813) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.788) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.959) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.817) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.833) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.756) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.959) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.782) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.818) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.755) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.966) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.822) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.960) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.762) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.955) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.809) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.770) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.776) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.813) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.805) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.965) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.779) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.805) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.835) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.824) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.964) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.779) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.972) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.843) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.817) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.791) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.827) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.831) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.965) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.791) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.832) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.818) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.981) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.825) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.988) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.855) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.836) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.832) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.862) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.843) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.987) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.831) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.985) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.863) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.858) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.848) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.984) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.838) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.990) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.989) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.870) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.986) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.853) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.858) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.843) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.985) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.869) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.853) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.830) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.836) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.839) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.824) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.862) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.847) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.825) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.835) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.982) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.868) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.855) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.825) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.855) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:12:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:12:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.849) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.849) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.988) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.875) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.861) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.881) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.841) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.867) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.841) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 1.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.857) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.865) total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:15:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best parameters:\n",
      "{'colsample_bytree': 0.8, 'gamma': 0.0, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.8, 'scale_pos_weight': np.float64(1.2289416846652268), 'subsample': 0.8}\n",
      "Best CV recall (class=1): 0.9353\n",
      "\n",
      "📋 Top 5 runs by recall:\n",
      "                                                 params  mean_train_score  \\\n",
      "762   {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...          0.997346   \n",
      "1018  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...          0.996606   \n",
      "510   {'colsample_bytree': 0.6, 'gamma': 0.1, 'learn...          0.997562   \n",
      "466   {'colsample_bytree': 0.6, 'gamma': 0.1, 'learn...          0.997161   \n",
      "243   {'colsample_bytree': 0.6, 'gamma': 0.0, 'learn...          0.996174   \n",
      "\n",
      "      mean_test_score  \n",
      "762          0.935267  \n",
      "1018         0.934434  \n",
      "510          0.934280  \n",
      "466          0.934064  \n",
      "243          0.933724  \n",
      "\n",
      "📊 Classification Report on TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      1156\n",
      "           1       0.76      0.87      0.81       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.84      0.85      0.84      1846\n",
      "weighted avg       0.86      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Base estimator\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Parameter grid around your new “best” run\n",
    "param_grid = {\n",
    "    'n_estimators':       [150, 200, 250],\n",
    "    'learning_rate':      [0.01, 0.03, 0.05],\n",
    "    'max_depth':          [4, 6, 8],\n",
    "    'min_child_weight':   [1, 3, 5],\n",
    "    'subsample':          [0.7, 0.9, 1.0],\n",
    "    'colsample_bytree':   [0.6, 0.8, 1.0],\n",
    "    'gamma':              [0.0, 0.1, 0.2],\n",
    "    'reg_alpha':          [0.0, 0.2, 0.4],\n",
    "    'reg_lambda':         [0.8, 1.0, 1.2],\n",
    "    # ratio = (# negative samples) / (# positive samples)\n",
    "    'scale_pos_weight':   [1, (len(y_combined) - sum(y_combined)) / sum(y_combined)]\n",
    "}\n",
    "\n",
    "# Make a scorer that focuses on recall for label=1\n",
    "recall_pos1 = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# 3. Grid Search (optimize for accuracy; swap scoring to 'recall' if you still want to boost class-1 recall)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator   = xgb_base,\n",
    "    param_grid  = param_grid,\n",
    "    scoring     = 'recall_pos1',\n",
    "    cv          = 2,\n",
    "    n_jobs      = -1,\n",
    "    verbose     = 1,\n",
    "    refit       = True\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Run the search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# 5. Best params & CV score\n",
    "print(\"🏆 Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# 6. Test‐set evaluation\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "print(\"📊 Classification Report on TEST:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Top results saved to 'grid_search_top_results.txt'\n"
     ]
    }
   ],
   "source": [
    "# Convert cv_results_ to DataFrame\n",
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort by validation recall (class 1)\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Select useful columns\n",
    "display_df = cv_results_df_sorted[[\n",
    "    'mean_train_score', 'mean_test_score', 'rank_test_score', 'params'\n",
    "]].rename(columns={\n",
    "    'mean_train_score': 'Train Recall',\n",
    "    'mean_test_score': 'Val Recall',\n",
    "    'rank_test_score': 'Rank'\n",
    "})\n",
    "\n",
    "# Format top 10 results\n",
    "top_results_text = display_df.head(10).to_string(index=False)\n",
    "\n",
    "# Save to text file\n",
    "with open(\"grid_search_top_results.txt\", \"w\") as f:\n",
    "    f.write(\"📋 Top 10 Grid Search Results by Validation Recall (class=1):\\n\\n\")\n",
    "    f.write(top_results_text)\n",
    "\n",
    "print(\"✅ Top results saved to 'grid_search_top_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 XGBoost - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      1156\n",
      "           1       0.76      0.87      0.81       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.84      0.85      0.84      1846\n",
      "weighted avg       0.86      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,             # Slightly reduced to prevent overfitting\n",
    "    learning_rate=0.05,           # Slower learning for better generalization\n",
    "    max_depth=6,                  # Shallower trees reduce overfitting\n",
    "    min_child_weight=5,           # Prevents learning from overly specific patterns\n",
    "    subsample=0.8,                # Keeps generalization\n",
    "    colsample_bytree=0.8,         # Same: helps with variance\n",
    "    gamma=0.0,                    # Adds split regularization\n",
    "    reg_alpha=0.2,                # L1 regularization\n",
    "    reg_lambda=0.8,               # L2 regularization\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (len(y_combined) - sum(y_combined)) / sum(y_combined)\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_combined)\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n📊 XGBoost - Classification Report:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_columns(csv_path):\n",
    "#     raw_columns = list(pd.read_csv(csv_path, nrows=0).columns)\n",
    "#     return [preprocess_keyword(col) for col in raw_columns]\n",
    "\n",
    "# # Step 1: Load and preprocess columns\n",
    "# columns_1 = extract_columns(\"Datasets/Banking/dataset1/loan_data.csv\")       # Dataset A\n",
    "# columns_2 = extract_columns(\"Datasets/Loans/dataset2/loan_data.csv\")  # Dataset B (note: different folder!)\n",
    "\n",
    "# # Step 2: Generate embeddings (assuming your function works correctly)\n",
    "# embeddings_1 = generate_embeddings(columns_1, tokenizer, model, device)\n",
    "# embeddings_2 = generate_embeddings(columns_2, tokenizer, model, device)\n",
    "\n",
    "# # Step 3: Create pair embeddings between each column from dataset 1 and dataset 2\n",
    "# Dataset_test = []\n",
    "# column_pairs = []\n",
    "\n",
    "# for i, emb1 in enumerate(embeddings_1):\n",
    "#     for j, emb2 in enumerate(embeddings_2):\n",
    "#         pair_embedding = np.concatenate([emb1, emb2])\n",
    "#         Dataset_test.append(pair_embedding)\n",
    "#         column_pairs.append((columns_1[i], columns_2[j]))\n",
    "\n",
    "# Dataset_test = np.array(Dataset_test)\n",
    "\n",
    "# # Step 4: Predict with trained model\n",
    "# Dataset_test_scaled = scaler.transform(Dataset_test)\n",
    "# predictions = svm_model.predict(Dataset_test_scaled)\n",
    "\n",
    "# # Step 5: Extract compatible pairs\n",
    "# compatible_pairs = [(col1, col2) for (col1, col2), pred in zip(column_pairs, predictions) if pred == 1]\n",
    "\n",
    "# # Step 6: Compatibility score based on bidirectional coverage\n",
    "# matched_A = set(col1 for col1, _ in compatible_pairs)\n",
    "# matched_B = set(col2 for _, col2 in compatible_pairs)\n",
    "\n",
    "# coverage_A = len(matched_A) / len(columns_1)\n",
    "# coverage_B = len(matched_B) / len(columns_2)\n",
    "\n",
    "# # Use harmonic mean for stricter measure (penalizes one-sided matching)\n",
    "# if coverage_A > 0 and coverage_B > 0:\n",
    "#     compatibility_score = hmean([coverage_A, coverage_B])\n",
    "# else:\n",
    "#     compatibility_score = 0.0\n",
    "\n",
    "# # Step 7: Print results\n",
    "# print(f\"\\nCompatibility Score: {compatibility_score:.2f}\")\n",
    "# print(\"Datasets are COMPATIBLE\" if compatibility_score >= 0.7 else \"Datasets are NOT compatible\")\n",
    "\n",
    "# print(\"\\n🔗 Compatible Column Pairs:\")\n",
    "# if compatible_pairs:\n",
    "#     for col1, col2 in compatible_pairs:\n",
    "#         print(f\"- {col1}  ↔️  {col2}\")\n",
    "# else:\n",
    "#     print(\"No compatible columns found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
