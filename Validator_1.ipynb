{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, make_scorer, recall_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Features = [\n",
    "    \"ticker\", \"company_name\", \"sector\", \"industry\", \"market_cap\",\n",
    "    \"price\", \"open\", \"close\", \"high\", \"low\",\n",
    "    \"volume\", \"adj_close\", \"dividend\", \"dividend_yield\", \"pe_ratio\",\n",
    "    \"eps\", \"beta\", \"52_week_high\", \"52_week_low\", \"shares_outstanding\",\n",
    "    \"float\", \"average_volume\", \"market\", \"exchange\", \"isin\",\n",
    "    \"cusip\", \"country\", \"currency\", \"ipo_date\", \"earnings_date\",\n",
    "    \"revenue\", \"cost_of_goods_sold\", \"gross_profit\", \"operating_expenses\", \"operating_income\",\n",
    "    \"ebit\", \"ebitda\", \"net_income\", \"income_before_tax\", \"tax_expense\",\n",
    "    \"net_income_applicable_to_common_shares\", \"basic_eps\", \"diluted_eps\", \"total_assets\", \"current_assets\",\n",
    "    \"non_current_assets\", \"total_liabilities\", \"current_liabilities\", \"non_current_liabilities\", \"shareholders_equity\",\n",
    "    \"retained_earnings\", \"cash_and_cash_equivalents\", \"short_term_investments\", \"long_term_investments\", \"inventory\",\n",
    "    \"accounts_receivable\", \"accounts_payable\", \"depreciation\", \"amortization\", \"capital_expenditures\",\n",
    "    \"loan_id\", \"loan_amount\", \"loan_term\", \"interest_rate\", \"installment\",\n",
    "    \"issue_date\", \"loan_status\", \"payment_status\", \"borrower_score\", \"borrower_income\",\n",
    "    \"debt_to_income\", \"employment_length\", \"purpose\", \"home_ownership\", \"delinquency_2yrs\",\n",
    "    \"credit_score\", \"fico_range_low\", \"fico_range_high\", \"revol_util\", \"num_open_credit_lines\",\n",
    "    \"total_credit_lines\", \"public_records\", \"collections_12_mths_ex_med\", \"application_type\", \"verification_status\",\n",
    "    \"bond_id\", \"bond_name\", \"maturity_date\", \"coupon_rate\", \"yield_to_maturity\",\n",
    "    \"face_value\", \"issue_price\", \"current_price\", \"duration\", \"convexity\",\n",
    "    \"credit_rating\", \"issuer\", \"callable\", \"puttable\", \"bond_type\",\n",
    "    \"transaction_id\", \"transaction_date\", \"transaction_amount\", \"transaction_type\", \"merchant_name\",\n",
    "    \"merchant_category\", \"account_id\", \"balance_before\", \"balance_after\", \"location\",\n",
    "    \"crypto_symbol\", \"crypto_name\", \"market_rank\", \"circulating_supply\", \"total_supply\",\n",
    "    \"max_supply\", \"market_dominance\", \"all_time_high\", \"all_time_low\", \"last_updated\",\n",
    "    \"block_time\", \"hashing_algorithm\", \"platform\", \"explorer_url\", \"trading_pairs\",\n",
    "    \"exchange_rate\", \"currency_pair\", \"base_currency\", \"quote_currency\", \"rate_date\",\n",
    "    \"rate_time\", \"daily_change\", \"monthly_change\", \"yearly_change\", \"volume_24h\",\n",
    "    \"investment_id\", \"investment_type\", \"investment_amount\", \"investment_date\", \"current_value\",\n",
    "    \"gain_loss\", \"annual_return\", \"investment_duration\", \"investment_strategy\", \"fund_manager\",\n",
    "    \"fund_id\", \"fund_name\", \"nav\", \"expense_ratio\", \"inception_date\",\n",
    "    \"fund_category\", \"assets_under_management\", \"benchmark_index\", \"turnover_ratio\", \"dividend_distribution\",\n",
    "    \"gdp\", \"inflation_rate\", \"unemployment_rate\", \"federal_funds_rate\", \"consumer_price_index\",\n",
    "    \"producer_price_index\", \"retail_sales\", \"housing_starts\", \"trade_balance\", \"government_debt\",\n",
    "    \"current_account_balance\", \"budget_deficit\", \"foreign_reserves\", \"money_supply\", \"taxpayer_id\",\n",
    "    \"income_bracket\", \"taxable_income\", \"effective_tax_rate\", \"tax_paid\", \"deductions\",\n",
    "    \"credits\", \"filing_status\", \"tax_year\", \"refund_amount\", \"bank_id\",\n",
    "    \"branch_id\", \"account_type\", \"account_open_date\", \"account_balance\", \"interest_earned\",\n",
    "    \"overdraft_limit\", \"minimum_balance\", \"monthly_fee\", \"account_status\", \"user_id\",\n",
    "    \"customer_id\", \"registration_date\", \"last_login\", \"kyc_status\", \"risk_score\",\n",
    "    \"fraud_flag\", \"device_id\", \"ip_address\", \"login_location\", \"portfolio_id\",\n",
    "    \"asset_class\", \"allocation_percentage\", \"benchmark_return\", \"tracking_error\", \"sharpe_ratio\",\n",
    "    \"alpha\", \"beta_coefficient\", \"standard_deviation\", \"max_drawdown\", \"audit_status\", \"accounting_standard\", \"financial_statement_type\", \"reporting_currency\", \"adjustment_reason\",\n",
    "    \"deferred_tax_assets\", \"deferred_tax_liabilities\", \"intangible_assets\", \"goodwill\", \"preferred_equity\",\n",
    "    \"policy_id\", \"policy_holder\", \"premium_amount\", \"coverage_amount\", \"claim_id\",\n",
    "    \"claim_status\", \"underwriting_score\", \"risk_class\", \"loss_ratio\", \"combined_ratio\",\n",
    "    \"order_id\", \"trade_price\", \"trade_volume\", \"order_type\", \"execution_time\",\n",
    "    \"bid_price\", \"ask_price\", \"spread\", \"order_book_depth\", \"trading_halt\",\n",
    "    \"swift_code\", \"iban\", \"routing_number\", \"account_opening_method\", \"branch_location\",\n",
    "    \"atm_withdrawals\", \"wire_transfers\", \"monthly_statements\", \"account_tier\", \"fee_structure\",\n",
    "    \"credit_limit\", \"credit_line_type\", \"charge_off_status\", \"days_past_due\", \"collection_agency\",\n",
    "    \"restructuring_status\", \"forbearance_flag\", \"loan_purpose\", \"collateral_type\", \"repayment_behavior\",\n",
    "    \"employment_rate\", \"labor_force_participation\", \"consumer_confidence_index\", \"housing_index\", \"manufacturing_index\",\n",
    "    \"import_volume\", \"export_volume\", \"interest_payment\", \"sovereign_rating\", \"external_debt\",\n",
    "    \"payment_method\", \"payment_gateway\", \"settlement_status\", \"refund_status\", \"dispute_id\",\n",
    "    \"chargeback_amount\", \"recurring_payment\", \"subscription_id\", \"billing_cycle\", \"invoice_date\",\n",
    "    \"wealth_segment\", \"advisor_id\", \"fee_schedule\", \"client_risk_profile\", \"discretionary_mandate\",\n",
    "    \"goals_based_plan\", \"financial_goal\", \"investment_objective\", \"cash_allocation\", \"equity_allocation\",\n",
    "    \"esg_score\", \"carbon_emission\", \"sustainability_rating\", \"board_diversity\", \"executive_compensation_ratio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Features = [\n",
    "   \"revenue_growth\", \"net_margin\", \"operating_margin\", \"book_value_per_share\", \"enterprise_value\",\n",
    "   \"ev_to_ebitda\", \"price_to_free_cash_flow\", \"fcf_margin\", \"roic\", \"roa\",\n",
    "   \"cash_conversion_cycle\", \"interest_coverage_ratio\", \"days_payable_outstanding\", \"inventory_turnover\", \"quick_ratio\",\n",
    "   \"z_score\", \"altman_z_score\", \"short_interest_ratio\", \"put_call_ratio\", \"analyst_recommendation\",\n",
    "   \"price_target_high\", \"price_target_low\", \"estimate_revision\", \"guidance_change\", \"buyback_yield\",\n",
    "   \"s&p_rating\", \"moody_rating\", \"recovery_rate\", \"default_probability\", \"credit_spread\",\n",
    "   \"real_interest_rate\", \"velocity_of_money\", \"consumer_sentiment_index\", \"labor_cost_index\", \"construction_spending\",\n",
    "   \"crypto_funding_rate\", \"staking_yield\", \"token_burn_rate\", \"dao_votes\", \"mining_difficulty\",\n",
    "   \"digital_wallet_id\", \"transaction_fee\", \"payment_token\", \"subscription_status\", \"auto_renew_flag\",\n",
    "   \"claim_frequency\", \"premium_to_coverage_ratio\", \"lapse_rate\", \"policy_duration\", \"actuarial_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define preprocessing function\n",
    "def preprocess_keyword(keyword):\n",
    "    return keyword.replace(\"_\", \" \").lower()\n",
    "\n",
    "\n",
    "# Load FinancialBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "# Function to generate embeddings using FinancialBERT\n",
    "def generate_embeddings(keywords, tokenizer, model, device):\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_Features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m keyword.replace(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m).lower()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Preprocess training and testing features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m processed_keywords = [preprocess_keyword(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mTrain_Features\u001b[49m]\n\u001b[32m      7\u001b[39m processed_testing_keywords = [preprocess_keyword(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m Test_Features]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load FinancialBERT tokenizer and model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Train_Features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess training and testing features\n",
    "processed_keywords = [preprocess_keyword(k) for k in Train_Features]\n",
    "processed_testing_keywords = [preprocess_keyword(k) for k in Test_Features]\n",
    "\n",
    "\n",
    "# Generate embeddings for training and testing data\n",
    "embeddings = generate_embeddings(processed_keywords, tokenizer, model, device)\n",
    "testing_embeddings = generate_embeddings(processed_testing_keywords, tokenizer, model, device)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Perform t-SNE visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "# Annotate each point with its corresponding label\n",
    "for i, label in enumerate(processed_keywords):\n",
    "    plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=7, alpha=0.8)\n",
    "\n",
    "# Add title and grid\n",
    "plt.title(\"t-SNE Visualization of Financial Keyword Embeddings using FinancialBERT\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping threshold 0.800 due to positive ratio: 0.2489\n",
      "‚ö†Ô∏è Skipping threshold 0.810 due to positive ratio: 0.2062\n",
      "‚ö†Ô∏è Skipping threshold 0.820 due to positive ratio: 0.1683\n",
      "‚ö†Ô∏è Skipping threshold 0.830 due to positive ratio: 0.1335\n",
      "‚ö†Ô∏è Skipping threshold 0.840 due to positive ratio: 0.1016\n"
     ]
    }
   ],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42, gamma='scale')\n",
    "\n",
    "threshold_candidates = np.arange(0.8, 0.9, 0.01)\n",
    "\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "\n",
    "min_positives_ratio = 0.01\n",
    "max_positives_ratio = 0.10\n",
    "\n",
    "\n",
    "for threshold in threshold_candidates:\n",
    "    X_temp, y_temp = [], []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            vec1, vec2 = embeddings[i], embeddings[j]\n",
    "            cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            label = 1 if cos_sim > threshold else 0\n",
    "            X_temp.append(np.concatenate([vec1, vec2]))\n",
    "            y_temp.append(label)\n",
    "\n",
    "    X_temp, y_temp = np.array(X_temp), np.array(y_temp)\n",
    "    positives_ratio = np.mean(y_temp)\n",
    "\n",
    "    # Skip thresholds with too few or too many positives\n",
    "    if not (min_positives_ratio <= positives_ratio <= max_positives_ratio):\n",
    "        print(f\"‚ö†Ô∏è Skipping threshold {threshold:.3f} due to positive ratio: {positives_ratio:.4f}\")\n",
    "        continue\n",
    "\n",
    "    # Split and scale\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "    X_val_scaled = scaler.transform(X_val_sub)\n",
    "\n",
    "    # Train and evaluate\n",
    "    svm_model.fit(X_train_scaled, y_train_sub)\n",
    "    y_pred_val = svm_model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_sub, y_pred_val, zero_division=0)\n",
    "\n",
    "    print(f\"‚úÖ Threshold: {threshold:.3f} | F1: {f1:.4f} | Positive Ratio: {positives_ratio:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "if best_threshold is not None:\n",
    "    print(f\"\\nüéØ Best Threshold Found: {best_threshold:.3f} with F1-score: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No valid threshold found within ratio constraints. Try adjusting the limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X = []  # Input features (concatenated embeddings)\n",
    "y = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        combined_features = np.concatenate([embeddings[i], embeddings[j]])  # Concatenate embeddings\n",
    "        X.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        y.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Original pairs: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 29169\n",
      "Original pairs: 43071\n"
     ]
    }
   ],
   "source": [
    "def jitter_embedding(embedding, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, embedding.shape)\n",
    "    return embedding + noise\n",
    "\n",
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(embeddings[i])\n",
    "                vec2_jit = jitter_embedding(embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X)}\")\n",
    "# Combine original and augmented data\n",
    "X_combined = np.vstack([X, np.array(augmented_pairs)])\n",
    "y_combined = np.concatenate([y, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for compatibility\n",
    "threshold = 0.85\n",
    "\n",
    "# Generate all possible pairs of features\n",
    "X_Test = []  # Input features (concatenated embeddings)\n",
    "y_test = []  # Labels (1 for compatible, 0 for incompatible)\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        combined_features = np.concatenate([testing_embeddings[i], testing_embeddings[j]])  # Concatenate embeddings\n",
    "        X_Test.append(combined_features)\n",
    "        \n",
    "        # Compute cosine similarity and assign label\n",
    "        cos_sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        y_test.append(1 if cos_sim > threshold else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_Test = np.array(X_Test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented pairs: 621\n",
      "Original pairs: 1225\n"
     ]
    }
   ],
   "source": [
    "augmented_pairs = []\n",
    "augmented_labels = []\n",
    "\n",
    "n_augments = 3  # Number of jittered copies per compatible pair\n",
    "\n",
    "for i in range(len(testing_embeddings)):\n",
    "    for j in range(i + 1, len(testing_embeddings)):\n",
    "        sim = cosine_similarity([testing_embeddings[i]], [testing_embeddings[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            for _ in range(n_augments):\n",
    "                vec1_jit = jitter_embedding(testing_embeddings[i])\n",
    "                vec2_jit = jitter_embedding(testing_embeddings[j])\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([testing_embeddings[i], vec2_jit]))\n",
    "                augmented_pairs.append(np.concatenate([vec1_jit, testing_embeddings[j]]))\n",
    "                augmented_labels.extend([1, 1, 1])\n",
    "\n",
    "\n",
    "print(f\"Augmented pairs: {len(augmented_pairs)}\")\n",
    "print(f\"Original pairs: {len(X_Test)}\")\n",
    "# Combine original and augmented data\n",
    "X_Test_combined = np.vstack([X_Test, np.array(augmented_pairs)])\n",
    "y_Test_combined = np.concatenate([y_test, np.array(augmented_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_combined)\n",
    "X_test_scaled = scaler.transform(X_Test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='f1',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"‚úîÔ∏è Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 90 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=59.2min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=60.0min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=60.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=60.4min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=60.7min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=60.8min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=60.9min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END .......................C=2, gamma=scale, kernel=rbf; total time=71.5min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=71.6min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.7min\n",
      "[CV] END .......................C=4, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=3, gamma=auto, kernel=rbf; total time=71.9min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=72.9min\n",
      "[CV] END .......................C=5, gamma=scale, kernel=rbf; total time=73.0min\n",
      "[CV] END .......................C=6, gamma=scale, kernel=rbf; total time=73.2min\n",
      "[CV] END ........................C=5, gamma=auto, kernel=rbf; total time=74.0min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=81.3min\n",
      "[CV] END ........................C=2, gamma=auto, kernel=rbf; total time=85.9min\n",
      "[CV] END .......................C=3, gamma=scale, kernel=rbf; total time=86.4min\n",
      "[CV] END ........................C=4, gamma=auto, kernel=rbf; total time=88.3min\n",
      "[CV] END ........................C=6, gamma=auto, kernel=rbf; total time=89.9min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=59.6min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=60.5min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=60.5min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=61.4min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=62.3min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=61.4min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=63.0min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=61.9min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=75.7min\n",
      "[CV] END ......................C=11, gamma=scale, kernel=rbf; total time=74.7min\n",
      "[CV] END ........................C=7, gamma=auto, kernel=rbf; total time=89.8min\n",
      "[CV] END .......................C=7, gamma=scale, kernel=rbf; total time=91.2min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=60.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=77.0min\n",
      "[CV] END .......................C=8, gamma=scale, kernel=rbf; total time=90.8min\n",
      "[CV] END ........................C=8, gamma=auto, kernel=rbf; total time=90.7min\n",
      "[CV] END .......................C=9, gamma=scale, kernel=rbf; total time=90.6min\n",
      "[CV] END .......................C=11, gamma=auto, kernel=rbf; total time=75.4min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=74.4min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=91.7min\n",
      "[CV] END ........................C=9, gamma=auto, kernel=rbf; total time=92.1min\n",
      "[CV] END ......................C=12, gamma=scale, kernel=rbf; total time=92.1min\n",
      "[CV] END .......................C=12, gamma=auto, kernel=rbf; total time=94.4min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=61.3min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.9min\n",
      "[CV] END ......................C=13, gamma=scale, kernel=rbf; total time=74.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=61.1min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=48.7min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=49.3min\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=57.7min\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END .......................C=14, gamma=auto, kernel=rbf; total time=74.4min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=45.1min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=61.7min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=76.5min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=62.0min\n",
      "[CV] END .......................C=13, gamma=auto, kernel=rbf; total time=94.0min\n",
      "[CV] END ............C=2, degree=2, gamma=scale, kernel=poly; total time=54.6min\n",
      "[CV] END .......................C=15, gamma=auto, kernel=rbf; total time=75.3min\n",
      "[CV] END ......................C=14, gamma=scale, kernel=rbf; total time=92.9min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=45.4min\n",
      "[CV] END ......................C=15, gamma=scale, kernel=rbf; total time=94.5min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=80.2min\n",
      "[CV] END .............C=2, degree=2, gamma=auto, kernel=poly; total time=55.1min\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=93.7min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=45.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ............C=3, degree=2, gamma=scale, kernel=poly; total time=55.4min\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=94.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=45.7min\n",
      "[CV] END .............C=3, degree=2, gamma=auto, kernel=poly; total time=56.4min\n",
      "[CV] END ............C=4, degree=2, gamma=scale, kernel=poly; total time=55.6min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=46.0min\n",
      "[CV] END .............C=4, degree=2, gamma=auto, kernel=poly; total time=56.1min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=82.0min\n",
      "[CV] END ............C=2, degree=3, gamma=scale, kernel=poly; total time=97.5min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=56.7min\n",
      "[CV] END ............C=5, degree=2, gamma=scale, kernel=poly; total time=45.9min\n",
      "[CV] END .............C=3, degree=3, gamma=auto, kernel=poly; total time=81.7min\n",
      "[CV] END .............C=2, degree=3, gamma=auto, kernel=poly; total time=99.2min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=45.3min\n",
      "[CV] END .............C=5, degree=2, gamma=auto, kernel=poly; total time=56.8min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=100.0min\n",
      "[CV] END ............C=4, degree=3, gamma=scale, kernel=poly; total time=82.2min\n",
      "[CV] END ...........C=3, degree=3, gamma=scale, kernel=poly; total time=101.7min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=4, degree=3, gamma=auto, kernel=poly; total time=82.4min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=45.2min\n",
      "[CV] END ............C=6, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=6, degree=2, gamma=auto, kernel=poly; total time=57.2min\n",
      "[CV] END ...........C=4, degree=3, gamma=scale, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=3, degree=3, gamma=auto, kernel=poly; total time=123.5min\n",
      "[CV] END ............C=4, degree=3, gamma=auto, kernel=poly; total time=104.0min\n",
      "[CV] END ............C=5, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=44.7min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END .............C=5, degree=3, gamma=auto, kernel=poly; total time=82.6min\n",
      "[CV] END ............C=7, degree=2, gamma=scale, kernel=poly; total time=57.9min\n",
      "[CV] END .............C=7, degree=2, gamma=auto, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ...........C=5, degree=3, gamma=scale, kernel=poly; total time=104.9min\n",
      "[CV] END ............C=6, degree=3, gamma=scale, kernel=poly; total time=82.3min\n",
      "[CV] END ............C=5, degree=3, gamma=auto, kernel=poly; total time=105.8min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=45.1min\n",
      "[CV] END ............C=8, degree=2, gamma=scale, kernel=poly; total time=57.7min\n",
      "[CV] END .............C=8, degree=2, gamma=auto, kernel=poly; total time=56.6min\n",
      "[CV] END ...........C=6, degree=3, gamma=scale, kernel=poly; total time=105.4min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=44.4min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=100.2min\n",
      "[CV] END .............C=7, degree=3, gamma=auto, kernel=poly; total time=81.9min\n",
      "[CV] END ............C=9, degree=2, gamma=scale, kernel=poly; total time=57.2min\n",
      "[CV] END .............C=9, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=44.6min\n",
      "[CV] END ............C=6, degree=3, gamma=auto, kernel=poly; total time=127.7min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=100.3min\n",
      "[CV] END ...........C=7, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ............C=8, degree=3, gamma=scale, kernel=poly; total time=81.8min\n",
      "[CV] END .............C=8, degree=3, gamma=auto, kernel=poly; total time=81.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=44.0min\n",
      "[CV] END ............C=7, degree=3, gamma=auto, kernel=poly; total time=105.1min\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=58.0min\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=58.5min\n",
      "[CV] END ...........C=8, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=44.1min\n",
      "[CV] END ............C=8, degree=3, gamma=auto, kernel=poly; total time=106.3min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=9, degree=3, gamma=scale, kernel=poly; total time=98.7min\n",
      "[CV] END ............C=11, degree=2, gamma=auto, kernel=poly; total time=57.3min\n",
      "[CV] END ...........C=9, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END .............C=9, degree=3, gamma=auto, kernel=poly; total time=99.6min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=44.2min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=43.8min\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=81.3min\n",
      "[CV] END ............C=9, degree=3, gamma=auto, kernel=poly; total time=107.1min\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=81.2min\n",
      "[CV] END ...........C=12, degree=2, gamma=scale, kernel=poly; total time=57.6min\n",
      "[CV] END ............C=12, degree=2, gamma=auto, kernel=poly; total time=57.8min\n",
      "[CV] END ..........C=10, degree=3, gamma=scale, kernel=poly; total time=106.4min\n",
      "[CV] END ...........C=10, degree=3, gamma=auto, kernel=poly; total time=106.1min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=11, degree=3, gamma=scale, kernel=poly; total time=80.8min\n",
      "[CV] END ............C=11, degree=3, gamma=auto, kernel=poly; total time=81.4min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=43.7min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=43.7min\n",
      "[CV] END ............C=13, degree=2, gamma=auto, kernel=poly; total time=58.2min\n",
      "[CV] END ...........C=12, degree=3, gamma=scale, kernel=poly; total time=80.9min\n",
      "[CV] END ...........C=13, degree=2, gamma=scale, kernel=poly; total time=72.7min\n",
      "[CV] END ..........C=11, degree=3, gamma=scale, kernel=poly; total time=107.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=44.1min\n",
      "[CV] END ...........C=11, degree=3, gamma=auto, kernel=poly; total time=107.6min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=80.8min\n",
      "[CV] END ...........C=14, degree=2, gamma=scale, kernel=poly; total time=58.2min\n",
      "[CV] END ............C=14, degree=2, gamma=auto, kernel=poly; total time=55.6min\n",
      "[CV] END ..........C=12, degree=3, gamma=scale, kernel=poly; total time=105.2min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=36.5min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=34.5min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=71.5min\n",
      "[CV] END ............C=12, degree=3, gamma=auto, kernel=poly; total time=97.9min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=76.7min\n",
      "[CV] END ...........C=15, degree=2, gamma=scale, kernel=poly; total time=44.3min\n",
      "[CV] END ...........C=13, degree=3, gamma=scale, kernel=poly; total time=87.6min\n",
      "[CV] END ............C=15, degree=2, gamma=auto, kernel=poly; total time=52.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=71.4min\n",
      "[CV] END ............C=13, degree=3, gamma=auto, kernel=poly; total time=85.6min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=58.1min\n",
      "[CV] END ...........C=14, degree=3, gamma=scale, kernel=poly; total time=78.2min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=41.9min\n",
      "[CV] END ............C=14, degree=3, gamma=auto, kernel=poly; total time=65.8min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=45.6min\n",
      "[CV] END ...........C=15, degree=3, gamma=scale, kernel=poly; total time=46.9min\n",
      "[CV] END ............C=15, degree=3, gamma=auto, kernel=poly; total time=41.0min\n",
      "‚úîÔ∏è Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "‚úîÔ∏è Params: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8843 (+/- 0.1048)\n",
      "‚úîÔ∏è Params: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8677 (+/- 0.1252)\n",
      "‚úîÔ∏è Params: {'C': 2, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8675 (+/- 0.1254)\n",
      "‚úîÔ∏è Params: {'C': 3, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8604 (+/- 0.1344)\n",
      "‚úîÔ∏è Params: {'C': 3, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8603 (+/- 0.1345)\n",
      "‚úîÔ∏è Params: {'C': 4, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "‚úîÔ∏è Params: {'C': 4, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8576 (+/- 0.1390)\n",
      "‚úîÔ∏è Params: {'C': 5, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8553 (+/- 0.1431)\n",
      "‚úîÔ∏è Params: {'C': 5, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8552 (+/- 0.1431)\n",
      "‚úîÔ∏è Params: {'C': 6, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1459)\n",
      "‚úîÔ∏è Params: {'C': 6, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8528 (+/- 0.1458)\n",
      "‚úîÔ∏è Params: {'C': 7, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "‚úîÔ∏è Params: {'C': 7, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8505 (+/- 0.1484)\n",
      "‚úîÔ∏è Params: {'C': 8, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8495 (+/- 0.1495)\n",
      "‚úîÔ∏è Params: {'C': 8, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8493 (+/- 0.1496)\n",
      "‚úîÔ∏è Params: {'C': 9, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8484 (+/- 0.1507)\n",
      "‚úîÔ∏è Params: {'C': 9, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8485 (+/- 0.1506)\n",
      "‚úîÔ∏è Params: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "‚úîÔ∏è Params: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8479 (+/- 0.1516)\n",
      "‚úîÔ∏è Params: {'C': 11, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8478 (+/- 0.1522)\n",
      "‚úîÔ∏è Params: {'C': 11, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8477 (+/- 0.1521)\n",
      "‚úîÔ∏è Params: {'C': 12, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "‚úîÔ∏è Params: {'C': 12, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8476 (+/- 0.1524)\n",
      "‚úîÔ∏è Params: {'C': 13, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "‚úîÔ∏è Params: {'C': 13, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "‚úîÔ∏è Params: {'C': 14, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8472 (+/- 0.1528)\n",
      "‚úîÔ∏è Params: {'C': 14, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "‚úîÔ∏è Params: {'C': 15, 'gamma': 'scale', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "‚úîÔ∏è Params: {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'} | Recall: 0.8473 (+/- 0.1527)\n",
      "‚úîÔ∏è Params: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.9056 (+/- 0.0844)\n",
      "‚úîÔ∏è Params: {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.9055 (+/- 0.0846)\n",
      "‚úîÔ∏è Params: {'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1310)\n",
      "‚úîÔ∏è Params: {'C': 1, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1310)\n",
      "‚úîÔ∏è Params: {'C': 2, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8812 (+/- 0.1123)\n",
      "‚úîÔ∏è Params: {'C': 2, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8811 (+/- 0.1124)\n",
      "‚úîÔ∏è Params: {'C': 2, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8325 (+/- 0.1591)\n",
      "‚úîÔ∏è Params: {'C': 2, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8323 (+/- 0.1593)\n",
      "‚úîÔ∏è Params: {'C': 3, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1257)\n",
      "‚úîÔ∏è Params: {'C': 3, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8683 (+/- 0.1258)\n",
      "‚úîÔ∏è Params: {'C': 3, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8220 (+/- 0.1742)\n",
      "‚úîÔ∏è Params: {'C': 3, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8218 (+/- 0.1743)\n",
      "‚úîÔ∏è Params: {'C': 4, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1320)\n",
      "‚úîÔ∏è Params: {'C': 4, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8629 (+/- 0.1321)\n",
      "‚úîÔ∏è Params: {'C': 4, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1809)\n",
      "‚úîÔ∏è Params: {'C': 4, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8164 (+/- 0.1810)\n",
      "‚úîÔ∏è Params: {'C': 5, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8608 (+/- 0.1355)\n",
      "‚úîÔ∏è Params: {'C': 5, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8607 (+/- 0.1356)\n",
      "‚úîÔ∏è Params: {'C': 5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8122 (+/- 0.1859)\n",
      "‚úîÔ∏è Params: {'C': 5, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8121 (+/- 0.1859)\n",
      "‚úîÔ∏è Params: {'C': 6, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "‚úîÔ∏è Params: {'C': 6, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8579 (+/- 0.1392)\n",
      "‚úîÔ∏è Params: {'C': 6, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8093 (+/- 0.1890)\n",
      "‚úîÔ∏è Params: {'C': 6, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8092 (+/- 0.1892)\n",
      "‚úîÔ∏è Params: {'C': 7, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8572 (+/- 0.1408)\n",
      "‚úîÔ∏è Params: {'C': 7, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8571 (+/- 0.1408)\n",
      "‚úîÔ∏è Params: {'C': 7, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1915)\n",
      "‚úîÔ∏è Params: {'C': 7, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8074 (+/- 0.1914)\n",
      "‚úîÔ∏è Params: {'C': 8, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "‚úîÔ∏è Params: {'C': 8, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8557 (+/- 0.1425)\n",
      "‚úîÔ∏è Params: {'C': 8, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "‚úîÔ∏è Params: {'C': 8, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8071 (+/- 0.1922)\n",
      "‚úîÔ∏è Params: {'C': 9, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "‚úîÔ∏è Params: {'C': 9, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1429)\n",
      "‚úîÔ∏è Params: {'C': 9, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8069 (+/- 0.1926)\n",
      "‚úîÔ∏è Params: {'C': 9, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8068 (+/- 0.1926)\n",
      "‚úîÔ∏è Params: {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "‚úîÔ∏è Params: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1428)\n",
      "‚úîÔ∏è Params: {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "‚úîÔ∏è Params: {'C': 10, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8062 (+/- 0.1933)\n",
      "‚úîÔ∏è Params: {'C': 11, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "‚úîÔ∏è Params: {'C': 11, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8556 (+/- 0.1428)\n",
      "‚úîÔ∏è Params: {'C': 11, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "‚úîÔ∏è Params: {'C': 11, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8059 (+/- 0.1940)\n",
      "‚úîÔ∏è Params: {'C': 12, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "‚úîÔ∏è Params: {'C': 12, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8555 (+/- 0.1431)\n",
      "‚úîÔ∏è Params: {'C': 12, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "‚úîÔ∏è Params: {'C': 12, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "‚úîÔ∏è Params: {'C': 13, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1438)\n",
      "‚úîÔ∏è Params: {'C': 13, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8554 (+/- 0.1437)\n",
      "‚úîÔ∏è Params: {'C': 13, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "‚úîÔ∏è Params: {'C': 13, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8055 (+/- 0.1945)\n",
      "‚úîÔ∏è Params: {'C': 14, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "‚úîÔ∏è Params: {'C': 14, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8552 (+/- 0.1442)\n",
      "‚úîÔ∏è Params: {'C': 14, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "‚úîÔ∏è Params: {'C': 14, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8054 (+/- 0.1946)\n",
      "‚úîÔ∏è Params: {'C': 15, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8550 (+/- 0.1444)\n",
      "‚úîÔ∏è Params: {'C': 15, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8551 (+/- 0.1444)\n",
      "‚úîÔ∏è Params: {'C': 15, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "‚úîÔ∏è Params: {'C': 15, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'} | Recall: 0.8052 (+/- 0.1948)\n",
      "\n",
      "‚úÖ Best Parameters: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for multiple kernels\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': list(range(1, 16, 1)),\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean, std, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):\n",
    "    print(f\"‚úîÔ∏è Params: {params} | Recall: {mean:.4f} (+/- {std:.4f})\")\n",
    "\n",
    "# Output best model\n",
    "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "# model.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nüìä Classification Report (Test Set):\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1024 candidates, totalling 2048 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:16:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.939) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.941) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.942) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.952, test=0.956) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.709) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.780) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.726) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.729) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.731) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.732) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.766) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.786) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.738) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.780) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:18:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.944) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.730) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.960) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.956) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.708) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.780) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.768) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:20:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.954) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.965) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.963) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.755) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.754) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.801) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.738) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.796) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.754) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.807) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:21:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.752) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.942) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.798) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.763) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.957) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.787) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.954) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.764) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.734) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.949) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.966) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.736) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.752) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.801) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.791) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:24:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.939) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.937) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.718) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.765) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.736) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.958) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.939) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.714) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.781) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.947) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.767) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.737) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:25:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.726) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.782) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.950) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.965) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.749) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.753) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.948) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.809) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.747) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.952) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.746) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.785) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.793) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.760) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.741) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.965) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.803) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:29:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.956) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.951) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:30:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.962) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.973) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.800) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.830) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.973) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.849) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:31:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.827) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.828) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.813) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.834) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.831) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:32:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.971) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.810) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.817) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.979) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.973) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.839) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.832) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.828) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.805) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:34:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.822) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.842) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:35:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.843) total time= 4.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.827) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.860) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:36:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:37:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.843) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.829) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.846) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:38:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.835) total time= 4.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.830) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.854) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:39:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.845) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.829) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.837) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:40:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.812) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.801) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.845) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:41:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.970) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.807) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.822) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.976) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.838) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.824) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.969) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.972) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.812) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.811) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.839) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:43:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.977, test=0.968) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.824) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.968) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.816) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:44:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.976) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:45:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.830) total time= 4.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.820) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.859) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.975) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:46:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.974) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.839) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.821) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:47:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.838) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.976) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.823) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:48:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.766) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.960) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.787) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 4.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.959) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.981, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.967) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.814) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.970) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:49:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.981) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.835) total time= 4.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.812) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.773) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.982) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.827) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.769) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.849) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.819) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.841) total time= 4.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:50:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.970) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.806) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.962) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.779) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.817) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.809) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:51:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.959) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.819) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.967) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.795) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:52:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.798) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.807) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.840) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.805) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:53:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.844) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.968) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.826) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:54:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.840) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.973) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.794) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.790) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.801) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:55:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.834) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.821) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.960) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.814) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.768) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:56:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.822) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.804) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.961) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.960) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.764) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.779) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.962) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:57:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.780) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.782) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.824) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.787) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.970) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:58:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.968) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.806) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.797) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.976) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.830) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:59:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.965) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.968) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.794) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.803) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.840) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.968) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.831) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.794) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.975) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.800) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.835) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:00:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.829) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.969) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.803) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:01:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.973) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.974) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.842) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.805) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.977) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.838) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.857) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:03:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.976) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:04:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.850) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.845) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:05:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:06:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.982) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.852) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.874) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.854) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:07:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.867) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:08:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.872) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.858) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:09:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:10:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.839) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.843) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.854) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.855) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.869) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:11:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.836) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:12:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.819) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:13:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.856) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.977) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.846) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.836) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.843) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:14:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:15:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.863) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:15:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.846) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.870) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.881) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:16:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.859) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.844) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.873) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.855) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.860) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.829) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:17:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.863) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:18:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:18:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.726) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.854) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.729) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.942) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.765) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.731) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.864) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.939) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.709) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.952, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:19:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.766) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:20:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:20:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.933, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.732) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.786) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.944) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.730) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.708) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.940) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.960) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.780) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.956) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.768) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.954) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.755) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.754) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.801) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:22:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.963) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.754) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.738) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.952) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.807) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.962) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.755) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:23:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.798) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.752) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:24:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.949) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.737) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.966) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.801) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.791) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.940) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.787) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.954) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.933, test=0.938) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.764) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:25:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.719) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.938, test=0.718) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.765) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.932, test=0.941) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.736) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:26:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.714) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.781) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.767) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.937, test=0.947) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.932, test=0.941) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.737) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.726) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.782) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:27:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.951) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.749) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.753) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.809) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:28:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.949) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.747) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.747) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:29:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.939, test=0.952) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.939, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.760) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.739) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.803) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:30:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.963) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.800) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:31:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.973) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.829) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:32:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.853) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.827) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.827) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.814) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.833) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:33:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.811) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.840) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.971) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:35:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.804) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.835) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.825) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:36:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.867) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:37:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.841) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.830) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:38:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.977) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.831) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:39:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.852) total time= 4.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.838) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.977) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:40:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.808) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.982) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.853) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:41:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.971) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.804) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.976) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.825) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.977, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.816) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:43:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.806) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.977, test=0.968) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.824) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.811) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.810) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.986, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:44:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.842) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:45:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:45:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.823) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.818) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.974) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.826) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.820) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.976) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:47:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.854) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.836) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.821) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.848) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:48:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.847) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.961) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.975) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.763) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.785) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.814) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.832) total time= 3.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.962, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:49:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.981) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.773) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.769) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.856) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.819) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.806) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.842) total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:50:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.960, test=0.787) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.958, test=0.781) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.817) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.957) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.813) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:51:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.819) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.793) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.970) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.967) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.803) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:52:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.835) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.976) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.820) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.804) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.795) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:53:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.973) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.840) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.826) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.818) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.809) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.841) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.824) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.973) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:54:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.834) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.790) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.960) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.772) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.819) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:55:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.814) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.815) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.960) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.768) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.959, test=0.783) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.813) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.804) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:56:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.764) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.962) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.960) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.779) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.969) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.961) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.946, test=0.957) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.814) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.808) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.768) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.783) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.824) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:57:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.787) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.968) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.806) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.791) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.976, test=0.830) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:58:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.831) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.969) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.966) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.794) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.799) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.974) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.976) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.834) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.968) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.969) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:59:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.977, test=0.825) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.803) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.975) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.829) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.836) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.968) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:00:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.963, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.973) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.975, test=0.842) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.805) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:01:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.848) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.986) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.979) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.862) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.840) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.831) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.851) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:02:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.977) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.827) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.984) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.852) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.849) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:04:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.834) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.851) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:05:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.981) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.862) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:06:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.860) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.847) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.869) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.872) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.860) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.839) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.990) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.875) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.985) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.862) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.841) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.831) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:09:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.853) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.865) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.840) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:10:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.833) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:11:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.843) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.984) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.985) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.826) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.856) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.976) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.983) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.837) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.828) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:12:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.852) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:13:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.981) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.850) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.866) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:14:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.872) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.856) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.842) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.980) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.874) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:15:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.859) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.858) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.871) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.861) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:16:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.859) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.938) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.954) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.723) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.770) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:17:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.881) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.744) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.939) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.939) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.859) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.727) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.786) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:18:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.943) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.774) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.783) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:19:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.938) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.751) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.752) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.780) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.962) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:20:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:21:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.951) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.769) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.962) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.746) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.796) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.757) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.797) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:22:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.952) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.769) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.764) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.802) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:23:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.943) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.940) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.773) total time= 2.1min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.799) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.747) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.738) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.954) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.766) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:24:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.733) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.741) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.775) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.944) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.936) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.940, test=0.722) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.954) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.742) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.952) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.790) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:26:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.747) total time= 2.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.791) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.806) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:27:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.761) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.764) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.964) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.963) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.790) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.953) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.965) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:28:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.767) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:29:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.753) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.962) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.763) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.963) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.804) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:30:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.808) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.971) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.799) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.976) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.834) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.973) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.826) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:31:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.815) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.822) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.836) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:32:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.972) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.809) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.812) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.825) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:33:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.807) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.978, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.815) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.828) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:34:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.829) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:35:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.822) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.845) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.842) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.978) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:36:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.833) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.824) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.849) total time= 4.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.836) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:37:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.848) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:38:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.975) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.828) total time= 4.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.971) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.814) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.833) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.970) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:39:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.842) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.807) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.830) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.848) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.819) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:40:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.793) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.839) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.977) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:41:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.971) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.812) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.817) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.831) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:42:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.823) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.819) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.977) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.807) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.846) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:43:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:44:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:44:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.819) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.841) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:45:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.981) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.810) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:46:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.996, test=0.844) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.850) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.835) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:47:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.771) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.974) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.831) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.766) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.975) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.969) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.816) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.799) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.982) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:48:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.960, test=0.777) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 4.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.767) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.804) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.843) total time= 3.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:49:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.959) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.959) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.760) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.805) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.796) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.956) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.767) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:50:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.967) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.789) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.977) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.838) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:51:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.819) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.967) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.804) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.789) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:52:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.833) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.823) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.776) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.826) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.976) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.813) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:53:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.787) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.793) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.975) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.832) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.755) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.817) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.781) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.818) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:54:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.755) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.967) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.822) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.969) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:55:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.762) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.968) total time= 1.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.811) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.770) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.970) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.821) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.801) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:56:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.966) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.780) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.976) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.835) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.824) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:57:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.965) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.796) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.972) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.779) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.976) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.844) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.821) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.964) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:58:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.788) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.834) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.821) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [20:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.964) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.791) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.838) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.825) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:00:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.981) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.981) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.987) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.836) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.830) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.858) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.843) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.840) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.986) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:01:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.852) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.854) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:02:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.826) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.986) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.860) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.979) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.864) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:04:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.857) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.990) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.840) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.876) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.855) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:05:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.985) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.983) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.854) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.872) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:06:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.983) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.989) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.840) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.867) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.987) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.861) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.982) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:08:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.838) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.983) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.856) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.853) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:09:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.829) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.826) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.848) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:10:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.987, test=0.982) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.987, test=0.979) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.983) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.834) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.980) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.873) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.845) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.827) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:11:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.831) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.846) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.856) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:12:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.845) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:13:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.878) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.868) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:14:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.841) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.987) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.871) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.984) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.848) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.883) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.858) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:15:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.986) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.983) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.723) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.844) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.934, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.848) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:16:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.955) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.770) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.987) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.936, test=0.939) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.744) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.939) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.860) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.0, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.868) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.731) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.773) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.957) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.786) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:17:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.941) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.956) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.734) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.774) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.955) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:18:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.935) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.783) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.937, test=0.938) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.751) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.954, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.752) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.954, test=0.953) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.780) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:19:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.950) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.751) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.961) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.793) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:20:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.944, test=0.952) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.769) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.959, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.745) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.796) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.954) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.954) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.795) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.952, test=0.762) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.962, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.757) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:21:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.797) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.802) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.950) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.953) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.769) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.963) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.763) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:22:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.802) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.964) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.787) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.734) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.943) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.936, test=0.941) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.944, test=0.737) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.773) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.953, test=0.957) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.953, test=0.955) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.934, test=0.938) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.747) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.943) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.799) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.955, test=0.955) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:23:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.766) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.735) total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.955, test=0.954) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.733) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.943) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.938) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.940, test=0.741) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.958, test=0.775) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.957) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:24:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.956) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.935, test=0.942) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.941, test=0.721) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.935, test=0.936) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.742) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.956, test=0.953) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.956, test=0.952) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.957, test=0.776) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.957, test=0.790) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:25:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.943, test=0.951) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.758) total time= 2.7min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.959, test=0.964) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.958, test=0.963) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.749) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.791) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.806) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:26:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.762) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.951) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.752) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.789) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.790) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.942, test=0.953) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:27:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.942, test=0.953) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.951, test=0.756) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.965) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.950, test=0.767) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.961, test=0.964) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.793) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.943, test=0.955) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:28:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.941, test=0.951) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.962) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.950, test=0.751) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.763) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.795) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.960, test=0.964) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.805) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:29:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.972) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.802) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.831) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.830) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:30:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.974) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.814) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.976) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.837) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:31:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.823) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.972) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.988, test=0.809) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.973) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.830) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:32:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.810) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.971) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.814) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.980) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.996, test=0.826) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:33:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.828) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.979) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:34:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.821) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.822) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.844) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:35:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.831) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.829) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.981) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.848) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.839) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:36:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.978) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.830) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.841) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:37:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.982, test=0.977) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.976) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:38:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.826) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.970) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.831) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.816) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.811) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.979) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.844) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.831) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.843) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:39:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.979, test=0.971) total time= 2.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.827) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.811) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.981, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.796) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.989, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.839) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.988, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:40:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.832) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.980, test=0.972) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.812) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.987, test=0.978) total time= 2.8min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.979, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.801) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:41:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.978) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.978, test=0.970) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.832) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.980, test=0.969) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.824) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.818) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.988, test=0.978) total time= 3.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.810) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:42:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.843) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.987, test=0.977) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.833) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:43:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.834) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.828) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.982) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:44:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.841) total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.976) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.975) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.825) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.983) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:45:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.817) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.980) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.984, test=0.978) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.977) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.847) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.845) total time= 4.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.828) total time= 4.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.823) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.997, test=0.846) total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:46:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.839) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.771) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.983, test=0.976) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.766) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.832) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.982, test=0.975) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:47:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.816) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.971) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.983) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.946, test=0.957) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.993, test=0.826) total time= 4.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.775) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.959) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.989, test=0.981) total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.767) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.857) total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.818) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.810) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.03, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.997, test=0.846) total time= 3.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:48:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.958) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.774) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.960) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.964, test=0.967) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.760) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.970) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.805) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.796) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.947, test=0.956) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.948, test=0.958) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:49:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.767) total time= 2.0min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.776) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.969) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.965, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.802) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.814) total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.968, test=0.789) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:50:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.836) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.977) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.822) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.953, test=0.967) total time= 2.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.965) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.797) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:51:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.787) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.954, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.828) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.979, test=0.835) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.973) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.801) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.967, test=0.776) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.970, test=0.975) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.826) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.813) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:52:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.966) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.788) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.959) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.817) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.833) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.756) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.959) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.956, test=0.782) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.968) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:53:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.970, test=0.818) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.811) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.947, test=0.957) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.755) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.960, test=0.966) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.969, test=0.822) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.971) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.968, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:54:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.949, test=0.960) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.959, test=0.762) total time= 2.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.945, test=0.955) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.957, test=0.777) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.963, test=0.967) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.963, test=0.971) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.971, test=0.809) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.806) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.948, test=0.958) total time= 1.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.949, test=0.957) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.958, test=0.770) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.955, test=0.776) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.961, test=0.968) total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:55:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.813) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.964, test=0.969) total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.966, test=0.805) total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.953, test=0.965) total time= 2.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.966) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.965, test=0.779) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.968, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.964, test=0.805) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.835) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:56:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.824) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.955, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.952, test=0.964) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.966, test=0.779) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.792) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:57:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.965, test=0.972) total time= 2.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.843) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.957, test=0.967) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:05] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.976, test=0.817) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.951, test=0.965) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.969, test=0.791) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.967, test=0.974) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.965, test=0.796) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.967, test=0.975) total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.978, test=0.827) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.975, test=0.831) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:58:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.956, test=0.968) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.967, test=0.790) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.954, test=0.965) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.966, test=0.791) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.966, test=0.973) total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.969, test=0.974) total time= 2.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.977, test=0.832) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.974, test=0.818) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [21:59:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.983, test=0.981) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.825) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.988) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.855) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.861) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:00:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.836) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.832) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.862) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.850) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:01:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.843) total time= 3.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.978) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:03] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.990, test=0.987) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.831) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.990, test=0.985) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.863) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.858) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:02:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.980) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.848) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.837) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.985) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.855) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:03:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.984) total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.838) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.990) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.845) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.989) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.870) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:04:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.986) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.985) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.853) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.847) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.877) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:05:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.984) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.858) total time= 3.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.843) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:06:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.876) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.864) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.990, test=0.986) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.988, test=0.985) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.999, test=0.869) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:07:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.986, test=0.981) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.986, test=0.979) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.988) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.853) total time= 3.7min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.868) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.830) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=3, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.862) total time= 3.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.995, test=0.836) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.991, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.863) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.839) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:08:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.981) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.978) total time= 2.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.994, test=0.824) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.996, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.862) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.986) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:49] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:09:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.998, test=0.847) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.985, test=0.980) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.995, test=0.825) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.984, test=0.978) total time= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.991, test=0.984) total time= 2.6min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.997, test=0.835) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:10:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.985, test=0.979) total time= 2.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.989, test=0.982) total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.998, test=0.868) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.855) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.992, test=0.983) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.996, test=0.825) total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.994, test=0.832) total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.992, test=0.984) total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.855) total time= 2.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=150, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.844) total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:11:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:12:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:12:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.849) total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.849) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:01] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.988) total time= 3.4min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.993, test=0.987) total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.850) total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.879) total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:13:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.3min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.843) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.990, test=0.982) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.844) total time= 3.5min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.989) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.995, test=0.988) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.991, test=0.984) total time= 3.2min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.984) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.875) total time= 3.5min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.9;, score=(train=0.999, test=0.851) total time= 3.3min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 2.9min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.994, test=0.988) total time= 3.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.999, test=0.861) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=1.000, test=0.881) total time= 3.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1, subsample=0.8;, score=(train=0.997, test=0.841) total time= 3.4min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=0.8, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.867) total time= 3.1min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.993, test=0.985) total time= 2.1min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.8;, score=(train=0.998, test=0.841) total time= 2.2min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.998, test=0.848) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1, subsample=0.9;, score=(train=0.989, test=0.983) total time= 2.0min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=0.994, test=0.988) total time= 1.6min\n",
      "[CV 2/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.995, test=0.987) total time= 1.8min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.8;, score=(train=0.999, test=0.857) total time= 1.9min\n",
      "[CV 1/2] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.2, reg_lambda=1.0, scale_pos_weight=1.2289416846652268, subsample=0.9;, score=(train=1.000, test=0.865) total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis I/.venv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [22:15:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best parameters:\n",
      "{'colsample_bytree': 0.8, 'gamma': 0.0, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.8, 'scale_pos_weight': np.float64(1.2289416846652268), 'subsample': 0.8}\n",
      "Best CV recall (class=1): 0.9353\n",
      "\n",
      "üìã Top 5 runs by recall:\n",
      "                                                 params  mean_train_score  \\\n",
      "762   {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...          0.997346   \n",
      "1018  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...          0.996606   \n",
      "510   {'colsample_bytree': 0.6, 'gamma': 0.1, 'learn...          0.997562   \n",
      "466   {'colsample_bytree': 0.6, 'gamma': 0.1, 'learn...          0.997161   \n",
      "243   {'colsample_bytree': 0.6, 'gamma': 0.0, 'learn...          0.996174   \n",
      "\n",
      "      mean_test_score  \n",
      "762          0.935267  \n",
      "1018         0.934434  \n",
      "510          0.934280  \n",
      "466          0.934064  \n",
      "243          0.933724  \n",
      "\n",
      "üìä Classification Report on TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      1156\n",
      "           1       0.76      0.87      0.81       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.84      0.85      0.84      1846\n",
      "weighted avg       0.86      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Base estimator\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Parameter grid around your new ‚Äúbest‚Äù run\n",
    "param_grid = {\n",
    "    'n_estimators':       [150, 200, 250],\n",
    "    'learning_rate':      [0.01, 0.03, 0.05],\n",
    "    'max_depth':          [4, 6, 8],\n",
    "    'min_child_weight':   [1, 3, 5],\n",
    "    'subsample':          [0.7, 0.9, 1.0],\n",
    "    'colsample_bytree':   [0.6, 0.8, 1.0],\n",
    "    'gamma':              [0.0, 0.1, 0.2],\n",
    "    'reg_alpha':          [0.0, 0.2, 0.4],\n",
    "    'reg_lambda':         [0.8, 1.0, 1.2],\n",
    "    # ratio = (# negative samples) / (# positive samples)\n",
    "    'scale_pos_weight':   [1, (len(y_combined) - sum(y_combined)) / sum(y_combined)]\n",
    "}\n",
    "\n",
    "# Make a scorer that focuses on recall for label=1\n",
    "recall_pos1 = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# 3. Grid Search (optimize for accuracy; swap scoring to 'recall' if you still want to boost class-1 recall)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator   = xgb_base,\n",
    "    param_grid  = param_grid,\n",
    "    scoring     = 'recall_pos1',\n",
    "    cv          = 2,\n",
    "    n_jobs      = -1,\n",
    "    verbose     = 1,\n",
    "    refit       = True\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Run the search\n",
    "grid_search.fit(X_train_scaled, y_combined)\n",
    "\n",
    "# 5. Best params & CV score\n",
    "print(\"üèÜ Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# 6. Test‚Äêset evaluation\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "print(\"üìä Classification Report on TEST:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Top results saved to 'grid_search_top_results.txt'\n"
     ]
    }
   ],
   "source": [
    "# Convert cv_results_ to DataFrame\n",
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort by validation recall (class 1)\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Select useful columns\n",
    "display_df = cv_results_df_sorted[[\n",
    "    'mean_train_score', 'mean_test_score', 'rank_test_score', 'params'\n",
    "]].rename(columns={\n",
    "    'mean_train_score': 'Train Recall',\n",
    "    'mean_test_score': 'Val Recall',\n",
    "    'rank_test_score': 'Rank'\n",
    "})\n",
    "\n",
    "# Format top 10 results\n",
    "top_results_text = display_df.head(10).to_string(index=False)\n",
    "\n",
    "# Save to text file\n",
    "with open(\"grid_search_top_results.txt\", \"w\") as f:\n",
    "    f.write(\"üìã Top 10 Grid Search Results by Validation Recall (class=1):\\n\\n\")\n",
    "    f.write(top_results_text)\n",
    "\n",
    "print(\"‚úÖ Top results saved to 'grid_search_top_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä XGBoost - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87      1156\n",
      "           1       0.76      0.87      0.81       690\n",
      "\n",
      "    accuracy                           0.85      1846\n",
      "   macro avg       0.84      0.85      0.84      1846\n",
      "weighted avg       0.86      0.85      0.85      1846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,             # Slightly reduced to prevent overfitting\n",
    "    learning_rate=0.05,           # Slower learning for better generalization\n",
    "    max_depth=6,                  # Shallower trees reduce overfitting\n",
    "    min_child_weight=5,           # Prevents learning from overly specific patterns\n",
    "    subsample=0.8,                # Keeps generalization\n",
    "    colsample_bytree=0.8,         # Same: helps with variance\n",
    "    gamma=0.0,                    # Adds split regularization\n",
    "    reg_alpha=0.2,                # L1 regularization\n",
    "    reg_lambda=0.8,               # L2 regularization\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (len(y_combined) - sum(y_combined)) / sum(y_combined)\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_combined)\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nüìä XGBoost - Classification Report:\")\n",
    "print(classification_report(y_Test_combined, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonized Dataset1 columns: ['open', 'high', 'low', 'close', 'volume', 'vwap', 'timestamp', 'transactions', 'otc']\n",
      "Harmonized Dataset2 columns: ['open', 'high', 'low', 'close', 'volume', 'vwap', 'timestamp', 'transactions', 'otc']\n",
      "\n",
      "Compatibility Score: 0.78\n",
      "Datasets are COMPATIBLE\n",
      "\n",
      "üîó Compatible Column Pairs:\n",
      "- open ‚ÜîÔ∏è open\n",
      "- open ‚ÜîÔ∏è close\n",
      "- open ‚ÜîÔ∏è volume\n",
      "- open ‚ÜîÔ∏è vwap\n",
      "- open ‚ÜîÔ∏è transactions\n",
      "- high ‚ÜîÔ∏è close\n",
      "- close ‚ÜîÔ∏è close\n",
      "- volume ‚ÜîÔ∏è high\n",
      "- volume ‚ÜîÔ∏è close\n",
      "- volume ‚ÜîÔ∏è volume\n",
      "- volume ‚ÜîÔ∏è transactions\n",
      "- vwap ‚ÜîÔ∏è open\n",
      "- vwap ‚ÜîÔ∏è volume\n",
      "- vwap ‚ÜîÔ∏è vwap\n",
      "- vwap ‚ÜîÔ∏è transactions\n",
      "- timestamp ‚ÜîÔ∏è vwap\n",
      "- timestamp ‚ÜîÔ∏è timestamp\n",
      "- timestamp ‚ÜîÔ∏è transactions\n",
      "- transactions ‚ÜîÔ∏è open\n",
      "- transactions ‚ÜîÔ∏è close\n",
      "- transactions ‚ÜîÔ∏è volume\n",
      "- transactions ‚ÜîÔ∏è vwap\n",
      "- transactions ‚ÜîÔ∏è transactions\n",
      "\n",
      "‚úÖ Embeddings and compatibility data saved to 'compatibility_analysis.pkl'\n"
     ]
    }
   ],
   "source": [
    "xgb_model = pickle.load(open('model.pkl', 'rb'))\n",
    "scaler = StandardScaler()\n",
    "# ‚îÄ‚îÄ Your helper functions ‚îÄ‚îÄ\n",
    "\n",
    "def preprocess_keyword(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a column name for comparison.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        s.lower()\n",
    "         .strip()\n",
    "         .replace(\"_\", \" \")\n",
    "         .replace(\"-\", \" \")\n",
    "         .replace(\"#\", \" \")\n",
    "         .replace(\"@\", \" \")\n",
    "    )\n",
    "\n",
    "def extract_columns(csv_path: str):\n",
    "    \"\"\"\n",
    "    Read just the header row and return \n",
    "    (preprocessed list, raw list) of column names.\n",
    "    \"\"\"\n",
    "    raw = list(pd.read_csv(csv_path, nrows=0).columns)\n",
    "    pre = [preprocess_keyword(c) for c in raw]\n",
    "    return pre, raw\n",
    "\n",
    "def build_subset_mapping(src_pre, tgt_pre):\n",
    "    \"\"\"\n",
    "    Map each src_pre ‚Üí the longest tgt_pre that strictly contains it.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for s in src_pre:\n",
    "        matches = [t for t in tgt_pre if s != t and s in t]\n",
    "        if matches:\n",
    "            mapping[s] = max(matches, key=len)\n",
    "    return mapping\n",
    "\n",
    "# ‚îÄ‚îÄ Paths & header extraction ‚îÄ‚îÄ\n",
    "\n",
    "path1 = \"/home/g7/Desktop/Thesis I/Datasets/Ingestor_Datasets/DF_1.csv\"\n",
    "path2 = \"/home/g7/Desktop/Thesis I/Datasets/Ingestor_Datasets/DF_2.csv\"\n",
    "\n",
    "cols1_pp, cols1_raw = extract_columns(path1)\n",
    "cols2_pp, cols2_raw = extract_columns(path2)\n",
    "\n",
    "# ‚îÄ‚îÄ Subset‚Äêbased renaming ‚îÄ‚îÄ\n",
    "\n",
    "map1_pp = build_subset_mapping(cols1_pp, cols2_pp)  # A‚ÜíB\n",
    "map2_pp = build_subset_mapping(cols2_pp, cols1_pp)  # B‚ÜíA\n",
    "\n",
    "pp_to_raw1 = dict(zip(cols1_pp, cols1_raw))\n",
    "pp_to_raw2 = dict(zip(cols2_pp, cols2_raw))\n",
    "\n",
    "rename1 = {\n",
    "    pp_to_raw1[src]: pp_to_raw2[tgt]\n",
    "    for src, tgt in map1_pp.items()\n",
    "    if src in pp_to_raw1 and tgt in pp_to_raw2\n",
    "}\n",
    "rename2 = {\n",
    "    pp_to_raw2[src]: pp_to_raw1[tgt]\n",
    "    for src, tgt in map2_pp.items()\n",
    "    if src in pp_to_raw2 and tgt in pp_to_raw1\n",
    "}\n",
    "\n",
    "cols1_h = [rename1.get(c, c) for c in cols1_raw]\n",
    "cols2_h = [rename2.get(c, c) for c in cols2_raw]\n",
    "\n",
    "print(\"Harmonized Dataset1 columns:\", cols1_h)\n",
    "print(\"Harmonized Dataset2 columns:\", cols2_h)\n",
    "\n",
    "# ‚îÄ‚îÄ Embedding & matching pipeline ‚îÄ‚îÄ\n",
    "\n",
    "# 1) Generate embeddings (one arg!)\n",
    "embeddings_1 = generate_embeddings(cols1_h, tokenizer, model, device)\n",
    "embeddings_2 = generate_embeddings(cols2_h, tokenizer, model, device)\n",
    "\n",
    "# 2) Build pairwise embeddings + track pairs\n",
    "pair_embeddings = []\n",
    "column_pairs   = []\n",
    "for i, e1 in enumerate(embeddings_1):\n",
    "    for j, e2 in enumerate(embeddings_2):\n",
    "        pair_embeddings.append(np.concatenate([e1, e2]))\n",
    "        column_pairs.append((cols1_h[i], cols2_h[j]))\n",
    "pair_embeddings = np.vstack(pair_embeddings)\n",
    "\n",
    "# 3) Scale ‚Äî **fit** then transform\n",
    "pair_scaled = scaler.fit_transform(pair_embeddings)\n",
    "# If you already have a saved, pre-fitted scaler:\n",
    "# scaler = load(\"scaler.joblib\")\n",
    "# pair_scaled = scaler.transform(pair_embeddings)\n",
    "\n",
    "# 4) Predict\n",
    "preds = xgb_model.predict(pair_scaled)\n",
    "\n",
    "# 5) Extract compatible pairs\n",
    "compatible_pairs = [\n",
    "    (a, b)\n",
    "    for (a, b), p in zip(column_pairs, preds)\n",
    "    if p == 1\n",
    "]\n",
    "\n",
    "# 6) Compute coverage & harmonic‚Äêmean score\n",
    "matched_A = {a for a, _ in compatible_pairs}\n",
    "matched_B = {b for _, b in compatible_pairs}\n",
    "\n",
    "coverage_A = len(matched_A) / len(cols1_h)\n",
    "coverage_B = len(matched_B) / len(cols2_h)\n",
    "compatibility_score = (hmean([coverage_A, coverage_B])\n",
    "                       if coverage_A and coverage_B else 0.0)\n",
    "\n",
    "# 7) Report\n",
    "print(f\"\\nCompatibility Score: {compatibility_score:.2f}\")\n",
    "print(\"Datasets are COMPATIBLE\" if compatibility_score >= 0.7 \n",
    "      else \"Datasets are NOT compatible\")\n",
    "print(\"\\nüîó Compatible Column Pairs:\")\n",
    "if compatible_pairs:\n",
    "    for a, b in compatible_pairs:\n",
    "        print(f\"- {a} ‚ÜîÔ∏è {b}\")\n",
    "else:\n",
    "    print(\"No compatible columns found.\")\n",
    "\n",
    "output_data = {\n",
    "    'embeddings_1': embeddings_1,         \n",
    "    'embeddings_2': embeddings_2,          \n",
    "    'cols1_h': cols1_h,                    \n",
    "    'cols2_h': cols2_h,                    \n",
    "    'compatibility_score': compatibility_score \n",
    "}\n",
    "\n",
    "# Save to a pickle file\n",
    "with open('compatibility_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)\n",
    "\n",
    "print(\"\\n‚úÖ Embeddings and compatibility data saved to 'compatibility_analysis.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
